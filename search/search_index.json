{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the SoilWise Technical Documentation!","text":"<p>SoilWise Technical Documentation currently consists of the following sections:</p> <ol> <li>Technical Components</li> <li>APIs</li> <li>Infrastructure</li> <li>Glossary</li> <li>Printable version - where you find all sections composed in one page, that can be easily printed using Web Browser options</li> </ol>"},{"location":"#essential-terminology","title":"Essential Terminology","text":"<p>A full list of terms used within this Technical Documentation can be found in the Glossary. The most essential ones are defined as follows:</p> <ul> <li>(Descriptive) metadata:  Summary information describing digital objects such as datasets and knowledge resources.</li> <li> <p>Metadata record: An entry in e.g. a catalogue or abstracting and indexing service with summary information about a digital object.</p> </li> <li> <p>Data: A collection of discrete or continuous values that convey information, describing the quantity, quality, fact, statistics, other basic units of meaning, or simply sequences of symbols that may be further interpreted formally (Wikipedia).</p> </li> <li> <p>Dataset: (Also: Data set) A collection of data (Wikipedia).</p> </li> <li> <p>Knowledge:  Facts, information, and skills acquired through experience or education; the theoretical or practical understanding of a subject. SoilWise mainly considers explicit knowledge -- Information that is easily articulated, codified, stored, and accessed. E.g. via books, web sites, or databases. It does not include implicit knowledge (information transferable via skills) nor tacit knowledge (gained via personal experiences and individual contexts). Explicit knowledge can be further divided into semantic and structural knowledge:</p> <ul> <li>Semantic knowledge: Also known as declarative knowledge, refers to knowledge about facts, meanings, concepts, and relationships. It is the understanding of the world around us, conveyed through language. Semantic knowledge answers the \"What?\" question about facts and concepts.</li> <li>Structural knowledge: Knowledge about the organisation and interrelationships among pieces of information. It is about understanding how different pieces of information are interconnected. Structural knowledge explains the \"How?\" and \"Why?\" regarding the organisation and relationships among facts and concepts.</li> </ul> </li> <li>Knowledge resource: A digital object, such as a document, a web page, or a database, that holds relevant explicit knowledge.</li> </ul>"},{"location":"#release-notes","title":"Release notes","text":"Date Action 30. 9. 2024 v2.0 Released: For D2.1 Developed &amp; Integrated DM components, v1 D3.1 Developed &amp; Integrated KM components, v1 and D4.1 Repository infrastructure, components and APIs, v1 purposes 30. 9. 2024 Technical Components functionality updated according to first SoilWise repository prototype 27. 8. 2024 APIs section restructured 20. 8. 2024 Knowledge Graph component added 13. 8. 2024 Metadata Authoring component added 1. 7. 2024 Metadata Augmentation component added 30. 4. 2024 v1.0 Released: For D1.3 Architecture Repository v1 purposes 27. 3. 2024 Technical Components restructured according to the architecture from Brugges Technical Meeting 27. 3. 2024 v0.1 Released: Technical documentation based on the Consolidated architecture 10. 2. 2024 Technical Documentation was initialized"},{"location":"glossary/","title":"Glossary","text":"Abstracting and indexing service Abstracting and indexing service is a service, e.g. a search engine, that abstracts and indexes digital objects or metadata records, and provides matching and ranking functionality in support of information retrieval. Acceptance Criteria Acceptance Criteria can be used to judge if the resulting software satisfies the user's needs. A single user story/requirement can have multiple acceptance criteria. API Application programming interface (API) is a way for two or more computer programs to communicate with each other (source wikipedia) Application profile Application profile is a specification for data exchange for applications that fulfil a certain use case. In addition to shared semantics, it also allows for the imposition of additional restrictions, such as the definition of cardinalities or the use of certain code lists (source: purl.eu). Artificial Intelligence Artificial Intelligence (AI) is a field of study that develops and studies intelligent machines. It includes the fields rule based reasoning, machine learning and natural language processing (NLP). (source: wikipedia) Assimilation Assimilation is a term indicating the processes involved to combine multiple datasets with different origin into a common dataset, the term is somewhat similarly used in psychology as <code>incorporation of new concepts into existing schemes</code> (source: wikipedia). But is not well aligned with its usage in the data science community: <code>updating a numerical model with observed data</code> (source: wikipedia) ATOM ATOM is a standardised interface to exchange news feeds over the internet. It has been adopted by INSPIRE as a basic alternative to download services via WFS or WCS. Catalogue Catalogue or metadata registry is a central location in an organization where metadata definitions are stored and maintained (source: wikipedia) Code list Code list an enumeration of terms in order to constrain input and avoid errors (source: UN). Conceptual model Conceptual model or domain model represents concepts (entities) and relationships between them (source: wikipedia) Content negotiation Content negotiation refers to mechanisms that make it possible to serve different representations of a resource at the same URI (source: wikipedia) Controlled vocabulary Controlled vocabulary provides a way to organize knowledge for subsequent retrieval. A carefully selected list of words and phrases, which are used to tag units of information so that they are more easily retrieved by a search (source: Semwebtech). Vocabulary, unlike the dictionary and thesaurus, offers an in-depth analysis of a word and its usage in different contexts (source: learn grammar) Cordis Cordis is the primary source of results from EU-funded projects since 1990 Corpus Corpus (plural: Corpora) is a repository of text documents (knowledge resources); a body of works. Typically the input for information retrieval. CSW CSW Catalogue Service for the Web Data Data is a collection of discrete or continuous values that convey information, describing the quantity, quality, fact, statistics, other basic units of meaning, or simply sequences of symbols that may be further interpreted formally (Wikipedia). Data source Data source/provider is a provider of data resources. Data management Data management is the practice of collecting, organising, managing, and accessing data (for some purpose, such as decision-making). Dataset Dataset (Also: Data set) A collection of data (Wikipedia). Dataverse Dataverse is open source research data repository software Datacite Datacite is a non-profit organisation that provides persistent identifiers (DOIs) for research data. Datacite metadata scheme Datacite metadata schema a datamodel for metadata for scientific resources Digital exchange of soil-related data Digital exchange of soil-related data (ISO 28258:2013) presents a conceptual model of a common understanding of what soil profile data are Digital soil mapping Digital soil mapping is the creation and the population of a geographically referenced soil databases generated at a given resolution by using field and laboratory observation methods coupled with environmental data through quantitative relationships (source: wikipedia) Discovery service Discovery service is a concept from INSPIRE indicating a service type which enables discovery of resources (search and find). Typically implemented as CSW. Download service Download service is a concept from INSPIRE indicating a service type which enables download of a (subset of a) dataset. Typically implemented as WFS, WCS, SOS or Atom. DOI DOI a digital identifier of an object, any object \u2014 physical, digital, or abstract Encoding Encoding is the format used to serialise a resource to a file, common encodings are xml, json, turtle ESDAC ESDAC thematic centre for soil related data in Europe EUSO EUSO European Soil Observatory GDAL OGR GDAL and OGR are software packages widely used to interact with a variety of spatial data formats GML Geography Markup Language (GML) is an xml based standardised encoding for spatial data. GeoPackage GeoPackage a set of conventions for storing spatial data a SQLite database Geoserver Geoserver java based software package providing access to remote data through OGC services Global Soil Information System Global Soil Information System (GLOSIS) is an activity of FAO Global Soil Partnership enabling a federation of soil information systems and interoperable data sets  GLOSIS domain model GLOSIS domain model is an abstract, architectural component that defines how data are organised; it embodies a common understanding of what soil profile data are. GLOSIS Web Ontology GLOSIS Web Ontology is an implementation of the GLOSIS domain model using semantic technology GLOSIS Codelists GLOSIS Codelists is a series of codelists supporting the GLOSIS web ontology. Including the codelists as published in the FAO Guidelines for Soil Description (v2007), soil properties as collected by FAO GfSD and procedures as initally collected by Johan Leenaars. Glosolan Glosolan network to strengthen the capacity of laboratories in soil analysis and to respond to the need for harmonizing soil analytical data HALE Humboldt Alignment Editor (HALE) java based desktop software to compose and apply a data transformation to data Harmonization Harmonization is the process of transforming two datasets to a common model, a common projection, usage of common domain values and align their geometries Information retreival Information retreival (IR) is the task of identifying and retrieving information system resources (e.g. digital objects or metadata records) that are relevant to a search query. It includes searching for the information in a document, searching for the documents themselves, as well as searching for metadata describing the documents. Iteration An iteration is each development cycle (three foreseen within the SoilWise project) in the project. Each iteration can have phases. There are four phases per iteration focussing on co-design, development, integration and validation, demonstration. JRC JRC Joint Research Centre of the European Commission, its Directorate General. The JRC provides independent, evidence-based science and knowledge, supporting EU policies to positively impact society. Relevant policy areas within JRC are JRC Soil and JRC INSPIRE Knowledge <p>Knowledge is facts, information, and skills acquired through experience or education; the theoretical or practical understanding of a subject. SoilWise mainly considers explicit knowledge -- Information that is easily articulated, codified, stored, and accessed. E.g. via books, web sites, or databases. It does not include implicit knowledge (information transferable via skills) nor tacit knowledge (gained via personal experiences and individual contexts). Explicit knowledge can be further divided into semantic and structural knowledge.</p> <ul> <li>Semantic knowledge: Also known as declarative knowledge, refers to knowledge about facts, meanings, concepts, and relationships. It is the understanding of the world around us, conveyed through language. Semantic knowledge answers the \"What?\" question about facts and concepts.</li> <li>Structural knowledge: Knowledge about the organisation and interrelationships among pieces of information. It is about understanding how different pieces of information are interconnected. Structural knowledge explains the \"How?\" and \"Why?\" regarding the organisation and relationships among facts and concepts.</li> </ul> Knowledge graph Knowledge graph is a representation of a network of real-world entities -- such as objects, events, situations or concepts -- and the relationships between them. Typically the network is made up of nodes, edges, and labels. Both semantic and structural knowledge can be expressed, stored, searched, visualised, and explored as knowledge graphs. Knowledge resource Knowledge resource is a digital object, such as a document, a web page, or a database, that holds relevant explicit knowledge. Knowledge source Knowledge source/provider is a provider of knowledge resources. Knowledge management Knowledge managmenet is the practice of collecting, organising, managing, and accessing knowledge (for some purpose, such as as decision-making). LLM Large Language Model is typically a deep learning model based on the transformer architecture that has been trained on vast amounts of text data, usually from know collections scraped from the Internet. Mapserver Mapserver C based software package providing access to remote data through OGC services Metadata (Descriptive) metadata is a summary information describing digital objects such as datasets and knowledge resources. Metadata record Metadata record is an entry in e.g. a catalogue or abstracting and indexing service with summary information about a digital object. Metadata source Metadata source/provider is a provider of metadata. NLP Natural Language Processing is an interdisciplinary subfield of computer science and artificial intelligence, primarily concerned with providing computers with the ability to process data encoded in natural language. It is closely related to information retrieval, knowledge representation and computational linguistics. Observations and Measurements A conceptual model for Observations and Measurements (O&amp;M), also known as ISO19156 OGC API OGC API building blocks that can be used to assemble novel APIs for web access to geospatial content Ontology Ontology is a formal representation of the entities in a knowledge graph. Ontologies and knowledge graphs can be expressed in a similar manner and they are closely related. Ontologies can be seen as the (semantic) data model defining classes, relationships and attributes, while knowledge graphs contain the real data according to the (semantic) data model. Persistent identifier Persistent identifier is a long-lasting reference to a digital object. Product backlog Product backlog is the document where user stories/requirements are gathered with their acceptance criteria, and prioritized. QGIS QGIS desktop software package to create spatial vizualisations of various types of data RAG Retrieval Augmented Generation is a framework for retrieving facts from an external knowledge base to ground large language models on the most accurate, up-to-date information and enhancing the (pre)trained parameteric (semantic) knowledge with non-parameteric knowledge to avoid hallucinations and get better responses. REA REA is the European Research Executive Agency, it's mandate is to manage several EU programmes and support services. Relational model Relational model an approach to managing data using a structure and language consistent with first-order predicate logic (source: wikipedia) RDF Resource Description Framework (RDF) a standard model for data interchange on the Web Representational state transfer Representational state transfer (REST) a set of guidelines for creating stateless, reliable web APIs (source: wikipedia) Requirements Requirements are the capabilities of an envisioned component of the repository which are classified as \u2018must have\u2019, or \u2018nice to have\u2019. Rolling plan Rolling plan is a methodology for considering the internal and external developments that may generate changes to the SoilWise Repository design and development. It keeps track of any developments and changes on a technical, stakeholder group level or at EUSO/JRC. SensorThings API SensorThingsAPI (STA) is a formalised protocol to exchange sensor data and tasks between IoT devices, maintained at Open Geospatial Consortium. Sensor Observation Service Sensor Observation Service (SOS) is a formalised protocol to exchange sensor data between entities, maintained at Open Geospatial Consortium. Sprint Sprint is a small timeframe during which tasks have been defined. Sprint backlog Sprint backlog is composed of the set of product backlog elements chosen for the sprint, and an action plan for achieving them. Soil classification Soil classification deals with the systematic categorization of soils based on distinguishing characteristics as well as criteria that dictate choices in use (source: wikipedia) Soilgrids Soilgrids a system for global digital soil mapping that uses many profile data and machine learning methods to predict the spatial distribution of soil properties across the globe  SoilWise Use cases The SoilWise use cases are described in the Grant Agreement to understand the needs from the stakeholder groups (users). Each use case provides user stories epics. Task Task is the smallest segment of work that must be done to complete a user story/requirement. UML Unified Modeling Language (UML) a general-purpose modeling language that is intended to provide a standard way to visualize the design of a system (source: wikipedia)  Usage scenarios Usage scenarios describe how (groups of) users might use the software product. These usage scenarios can originate or be updated from the SoilWise use cases, user story epic or user stories/requirements. User story A User story is a statement, written from the point of view of the user, that describes the functionality needed by the user from the SoilWise Repository.  User story epic A User story epic is a narrative of stakeholders needs that can be narrowed down into smaller specific needs (user stories/requirements). Validation framework Validation framework is a framework allowing good communication between users and developers, validation of developed products by users, and flexibility on the developer\u2019s side to take change requests into account as soon as possible. The validation framework needs a description of the functionalities to be developed (user stories/requirements), the criteria that enable to verify that the developed component corresponds to the user needs (acceptance criteria), the definition of tasks for the developers (backlog) and the workflow. View service View service is a concept from INSPIRE indicating a service type which presents a (pre)view of a dataset. Typically implemented as WMS or WMTS. Web service Web service a service offered by a device to another device, communicating with each other via the Internet (source: wikipedia) WOSIS WOSIS is a global dataset, maintained at ISRIC, aiming to serve the user with a selection of standardised and ultimately harmonised soil profile data WMS Web Map service (WMS) is a formalised protocol to exchange geospatial data represented as images WFS Web Feature Service (WFS) is a formalised protocol to exchange geospatial vector data WCS Web Coverage Service (WCS)  is a formalised protocol to exchange geospatial grid data XSD XML Schema Definition (XSD) recommendation how to formally describe the elements in an Extensible Markup Language (XML) document (source: wikipedia)"},{"location":"governance/","title":"Certification &amp; Governance management (EV ILVO)","text":"<pre><code>flowchart LR\n    UM(User Management) --&gt; CG(\"`**Certification &amp; Governance management**`\")\n    DPU(Data &amp; Knowledge publication) --&gt; CG\n</code></pre> <p>??? question to allign on: is this governance related to:</p> <ul> <li>connecting with a data space, </li> <li>setting up the SWR as a data space </li> <li>or common usage of data and knowledge by users? In case of the latter are we not further distributing \"open data\"  and what extra governance policies do we need?</li> </ul> <p></p> <p></p> <p>SoilWise plans to implement a governance framework to tackle the challenge of ensuring equitable access to and  utilization of data and knowledge. This framework aims to foster data sharing and enable the generation of value  from these resources. A governance framework encompasses a set of principles, standards, policies (rules/regulations),  and practices. Additionally, the framework addresses the enforcement of these measures and the resolution of  any conflicts that may arise.</p> <p>The governance framework will be designed by integrating relevant EU legislation concerning governance within data  ecosystems, alongside insights from ongoing Digital Europe CSA projects focused on constructing the Common European  Data Spaces. The formulation of this governance framework will rely mainly on the Data Spaces Support Centre  (DSSC)  results which are:</p> <ol> <li>Starter Kit,  a document that helps organizations and individuals understand the requirements for creating a data space by providing.  a multifaceted view of data spaces, highlighting business, legal and governance, operational, functional,  and technical aspects to consider</li> <li>Glossary </li> <li>Blueprint,  a consistent and comprehensive set of guidelines to support the development cycle of data spaces.</li> </ol> <p>Additionally, the DSSC proposes the utilization of the Building Blocks Taxonomy, which serves as a classification scheme.  This taxonomy aids in describing, analyzing, and organizing data space initiatives based on predefined characteristics,  thus promoting a structured approach to governance implementation (Figure 1). We will equally consider the openDEI design principles for data spaces, the requirements of ISO 30401 for KM and rely on the results of the preparatory  action for the data space for Agriculture (AgriDataSpace).</p> <p></p> <p></p> <p>Following the introduction of the GDPR, the European Commission has put forward several legislative proposals,  such as the Digital Services Act, the Digital Markets Act, the Data Act, and the Data Governance Act.  SoilWise places particular emphasis on the Data Governance Act  and the Data Act, as their primary goals  align closely with the project's aims to enhance data sharing and facilitate product development.  These legislations are designed to:</p> <ul> <li>Promote equity in the distribution of value derived from data across various stakeholders.</li> <li>Enhance access to data and its utilization.</li> </ul> <p>The documents described in the higher paragraphs will be used to asses if the technical components used to develop and  implement SWR meet the necessities for the governance of the data ecosystem. If this is not the case technical  components adhering the governance requirements will be integrated in further iterations of the project.</p> <ul> <li>governance</li> <li>interoperability</li> <li>clearing house, broker, ...</li> <li> <p>vocabulary provider (connects to knowledge graph)</p> </li> <li> <p>connections with: external repo, identity providers, connectors, UI/UX</p> </li> </ul>"},{"location":"governance/#soilwise-data-spaces-ev-ilvo-we","title":"SoilWise Data Spaces (EV ILVO + WE)","text":"<p>T1.4 will produce detailed technical specifications, including information on components to be (re)used, interfaces between them and explaining the data flows and processing schemes, considering AgriDataSpace project conceptual reference architecture, AI/ML architecture patterns and the Ethics by Design in AI.</p>"},{"location":"governance/#-we-","title":"--- WE ---","text":"<p>A Data Space is a type of collaboration model defined as a decentralized infrastructure (where data is not stored centrally, but at the source) for trusted data sharing and exchange in data ecosystems, based on commonly agreed principles. There is no central repository into which data providers supply their data and from which consumers can access and retrieve data. Instead, data is exchanged directly between appropriate parties.</p> <p>Data Space facilitates the secure exchange, linkage, and interoperability of data within a confined ecosystem, based on standards and collaborative governance models, while preserving the digital sovereignty of data owners. Data spaces enable the use of data that may not be open but provides a certain level of accessibility.</p> <p></p> <p>Image credited to IDSA: The Reference Architecture Model</p>"},{"location":"governance/#components-design-of-a-data-space-based-on-the-idsa","title":"Components/ Design of a data space (based on the IDSA)","text":"<p>There are different approaches to designing data spaces, but the IDS-RAM (reference architecture model) of IDSA the International Data Space Association, which is characterized by an open, reliable and federated architecture for cross-sectoral data exchange, can be taken as a benchmark, containing at least a basic set of components necessary to build a robust data space.</p> <p>The most important components of Data Spaces are briefly described below.</p>"},{"location":"governance/#connectors","title":"Connectors","text":"<p>The Connector is the central technical component for secure and trusted data exchange, through which participants access data in a Data Space. It is handling the data according to policies defined by the data owner interms of access and usage rights, thus ensuring its sovereignty. IDS connectors for instance can publish the description of their data endpoints atan IDS meta-data broker. This allows potential data consumers to look up available data sources and data in terms of content, structure quality, actuality and other attributes (source: IDSA).</p> <p>Connectors can be certified in order to prevent malfunctionand to guarantee their integrity and compliance.\u00a0</p>"},{"location":"governance/#intermediaries","title":"Intermediaries","text":"<p>Intermediaries are services provided by third parties that are necessary for publishing, searching and registering transactions. Some of the intermediaries are:\u00a0</p> <p>Vocabulary providers</p> <p>Vocabulary providers manage and offer vocabularies and ontologies, reference data models and metadata to annotate and classify data sets, describe the datasets\u2019 relationships and define possible constraints. This allows data to be systematically organized, categorized and labelled, thus improving interoperability.</p> <p>Metadata broker</p> <p>According to IDSA, the Metadata Broker forms the reference implementation for registration and search functionality\u00a0compliant with International Data Spaces. As such, it follows the generic connector architecture described in the reference architecture model.</p> <p>Identity providers</p> <p>An identity provider is a system entity that creates, maintains, manages and validates identity information for clients and also provides authentication services for trusted applications within a federated or distributed network.\u00a0(Source: wiki).</p> <p>Clearing house</p> <p>Clearing house allows to keep control of the operations carried out. The IDS clearing house for instance provides decentralized and auditable traceability of all transactions if needed.</p> <p>CONNECTOR:</p> <ul> <li>config and control of data access</li> <li>config and control of data usage</li> <li> <p>user authorisation for data access</p> </li> <li> <p>connections with: Storage, APIs</p> </li> <li>technologies used: Eclipse Dataspace Components Connector</li> <li>responsible person: Thorsten Reitz</li> <li>participating:</li> </ul>"},{"location":"apis/apis-intro/","title":"Introduction","text":"<p>Within the first development iteration, the following APIs are employed in the SoilWise repository:</p> <ul> <li>Discovery APIs<ul> <li>SPARQL: https://sparql.soilwise-he.containers.wur.nl/sparql/</li> <li>OGC API- Records: https://soilwise-he.containers.wur.nl/cat/openapi</li> <li>Spatio Temporal Asset Catalog (STAC): https://soilwise-he.containers.wur.nl/cat/stac/openapi</li> <li>Catalog service for the Web (CSW): https://soilwise-he.containers.wur.nl/cat/openapi</li> <li>Protocol for Metadata Harvesting (OAI-PMH): https://soilwise-he.containers.wur.nl/cat/oaipmh</li> <li>OpenSearch: https://soilwise-he.containers.wur.nl/cat/opensearch</li> </ul> </li> <li>Processing API's<ul> <li>Translate API: https://api.soilwise-he.containers.wur.nl/tolk/docs</li> <li>Link Liveness Assessment API: https://api.soilwise-he.containers.wur.nl/linky/docs</li> <li>RDF to triplestore API: https://repo.soilwise-he.containers.wur.nl/swagger-ui/index.html</li> </ul> </li> </ul>"},{"location":"apis/apis-intro/#future-work","title":"Future work","text":"<p>SoilWise will in the future use more APIs to interact between components as well as enable remote users to interact with SoilWise components. Standardised APIs will be used if possible, such as:</p> <ul> <li>Open API</li> <li>GraphQL</li> <li>OGC webservices (preferably OGC API generation based on Open API)</li> <li>SPARQL for potential future knowledge graphs</li> </ul>"},{"location":"infrastructure/containerization/","title":"Containerization","text":"<p>The SWR is being developed in a containerised docker environment. This means that each software component, whether it's a database, storage system, or some kind of service, is compiled into a container image. These images are made available in a hub or repository, so that they can be deployed automatically whenever needed, including to fresh hardware.</p>"},{"location":"infrastructure/git/","title":"GIT versioning system","text":"<p>All aspects of the SoilWise repository can be managed through the SoilWise GitHub repository.  This allows all members of the Mission Soil and EUSO community to provide feedback or contribute to any of the aspects.</p>"},{"location":"infrastructure/git/#documentation","title":"Documentation","text":"<p>Documentation is maintained in the markdown format using McDocs and deployed as html or pdf using GitHub Pages.</p> <p>An interactive preview of architecture diagrams is also maintained and published using GitHub Pages.</p>"},{"location":"infrastructure/git/#source-code","title":"Source code","text":"<p>Software libraries tailored or developed in the scope of SoilWise are maintained through the GitHub repository.</p>"},{"location":"infrastructure/git/#container-build-scriptsdeployments","title":"Container build scripts/deployments","text":"<p>SoilWise is based on an orchestrated set of container deployments. Both the definitions of the containers as well as the orchestration of those containers are maintained through Git.</p>"},{"location":"infrastructure/git/#harvester-definitions","title":"Harvester definitions","text":"<p>The configuration of the endpoint to be harvested, filters to apply and the interval is stored in a GitHub repository. If the process runs as a CI-CD pipeline, then the logs of each run are also available in Git.</p>"},{"location":"infrastructure/git/#authored-and-harvested-metadata","title":"Authored and harvested metadata","text":"<p>Metadata created in SWR, as well as metadata imported from external sources, are stored in GitHub, so a full history of each record is available, and users can suggest changes to existing metadata.</p>"},{"location":"infrastructure/git/#validation-rules","title":"Validation rules","text":"<p>Rules (ATS/ETS) applied to metadata (and data) validation are stored in a git repository.</p>"},{"location":"infrastructure/git/#etl-configuration","title":"ETL configuration","text":"<p>Alignments to be applied to the source to be standardised and/or harmonised are stored on a git repository, so users can try the alignment locally or contribute to its development.</p>"},{"location":"infrastructure/git/#backlog-discussions","title":"Backlog / discussions","text":"<p>Roadmap discussion, backlog and issue management are part of the GitHub repository. Users can flag issues on existing components, documentation or data, which can then be followed up by the participants.</p>"},{"location":"infrastructure/git/#release-management","title":"Release management","text":"<p>Releases of the components and infrastructure are managed from a GitHub repository, so users understand the status of a version and can download the packages. The release process is managed in an automated way through CI-CD pipelines.</p>"},{"location":"infrastructure/infrastructure-intro/","title":"Introduction","text":"<p>This section describes the general hardware infrastructure and deployment pipelines used for the SWR. As of the delivery of this initial version of the technical documentation, a prototype pipeline and hardware environment shall continuously be improved as required to fit the needs of the project.</p> <p>For the development of First project iteration cycle, we defined the following criteria:</p> <ul> <li>There is no production environment.</li> <li>There is a distributed staging environment, with each partner deploying their solutions to their specific hardware.</li> <li>All of the hardware nodes used in the staging environment include an offsite backup capacity, such as a storage box, that is operated in a different physical location.</li> <li>There is no central dev/test environment. Each organisation is responsible for its own dev/test environments.</li> <li>The deployment and orchestration configuration for this iteration should be stored as YAML in a GitHub repository.</li> <li>Deployments to the distributed staging environment are done preferably through GitHub Actions or through alternative pipelines, such as a Jenkins or GitLab instance provided by weTransform or other partners.</li> <li>For each component, there shall be separate build processes managed by the responsible partners that result in the built images being made accessible through a hub (e.g. dockerhub)</li> </ul>"},{"location":"infrastructure/infrastructure-intro/#work-completed-iteration-1","title":"Work completed - Iteration 1","text":"<p>The Soilwise infrastructure uses components provided by Github. Github components are used to:</p> <ul> <li>Administer and assign to roles the different Soilwise users.</li> <li>Register, prioritise and assign tasks.</li> <li>Store source code of software artifacts.</li> <li>Author documentation.</li> <li>Run CI/CD pipelines.</li> <li>Collect user feedback.</li> </ul> <p>During the iteration, the following components have been deployed:</p> <p>on infrastructure provided by Wageningen University:</p> <ul> <li>A PostGreSQL database on the PostGreSQL cluster.</li> <li>A number of repositories at the university Gitlab instance, including CI/CD pipelines to run metadata harvesters.</li> <li>A range of services deployed on the univerity k8s cluster, with their configuration stored on Gitlab. Container images are stored on the university Harbor repository.</li> <li>Usage logs monitored through the university instance of Splunk.</li> <li>Availability monitoring provided by Uptimerobot.com.</li> </ul> <p>on WeTransform cloud infrastructure:</p> <ul> <li>a k8s deployment of the hale connect stack as been installed and configured. This instance can provide user management and has been integrated with the GitHub repository https://github.com/soilwise-he/Soilwise-credentials. The stack provides Transformation, Metadata Generation and Validation capabilities.</li> </ul>"},{"location":"infrastructure/infrastructure-intro/#future-work-iteration-2","title":"Future work - Iteration 2","text":"<p>The main objective of iteration 2 is to reorganise the orchestration of the different components, so all components can be centrally accessed and monitored.  </p> <p>The integrations will, whereever feasible, build on API's which are standardised by W3C, OGC or de facto standards, such as Open API or GraphQL. </p> <p>The intent of the consortium is to set up a distributed architecture, with the staging and production environment in an overall kubernetes-based orchestration mode if it is deemed necessary and advantageous at that point in time.</p>"},{"location":"technical_components/catalogue/","title":"Catalogue","text":"<p>Info</p> <p>Current version: 0.1.0</p> <p>Project: pycsw</p> <p>Access point: https://soilwise-he.containers.wur.nl/cat/</p> <p>The metadata catalogue is a central piece of the architecture,  giving access to individual metadata records. In the catalogue domain, various effective metadata catalogues are developed around the standards issued by the OGC, the Catalogue Service for the Web (CSW) and the OGC API Records, Open Archives Initiative (OAI-PMH), W3C (DCAT), FAIR science (Datacite) and Search Engine community (schema.org). For our first iteration we've selected the pycsw software, which supports most of these standards. </p>"},{"location":"technical_components/catalogue/#functionality","title":"Functionality","text":"<p>The SoilWise prototype adopts a frontend, focusing on:</p> <ul> <li>minimalistic User Interface, to prevent a technical feel,</li> <li>paginated search results, sorted alphabetically, by date, see more information in Chapter Query Catalogue,</li> <li>option to filter by facets, see more information in Chapter Query Catalogue,</li> <li>preview of the dataset (if a thumbnail or OGC:Service is available), else display of its spatial extent, see more information in Chapter Display record's detail,</li> <li>option to provide feedback to publisher/author, see more information in Chapter User Engagement,</li> <li>readable link in the browser bar, to facilitate link sharing.</li> </ul>"},{"location":"technical_components/catalogue/#query-catalogue","title":"Query Catalogue","text":"<p>The SoilWise Catalogue currently enables the following search options:</p> <ul> <li>fulltext search</li> <li>faceted search</li> </ul> <p>50 results are displayed per page in alphabetical order, in the form of overview table comprising preview of title, abstract, contributor, type and date. Search items set through user interface is also reflected in the URL to facilitate sharing.</p>"},{"location":"technical_components/catalogue/#fulltext-search","title":"Fulltext search","text":"<p>Fulltext search is currently enabled through the q= attribute. Other queryable parameters are title, keywords, abstract, contributor. Full list of queryables can be found at: https://soilwise-he.containers.wur.nl/cat/collections/metadata:main/queryables.</p> <p>Fulltext search currently supports only nesting words with AND operator.</p>"},{"location":"technical_components/catalogue/#faceted-search","title":"Faceted search","text":"<ul> <li>filter by physical soil parameters (soil texture, WRB, soil structure, bulk density, porosity, water holding capacity, soil moisture),</li> <li>filter by chemical soil parameters (ph, organic matter, cation exchange capacity, electrical conductivity, nutrient content, soil carbon, soil nitrogen, soil phosporus, heavy metals concentration),</li> <li>filter by biological soil parameters (microbial biomass, soil enzyme activities, soil fauna, soil respiration),</li> <li>filter by soil functions (soil fertility, water regulation, soil erosion control, carbon sequestration, soil health, supporting plant growth, contaminant filtration),</li> <li>filter by soil degradation indicators (soil erosion, soil compaction, soil salinization, soil acidification, soil contamination),</li> <li>filter by environmental soil functions (habitat for organisms, climate regulation, water filtration),</li> <li>fitler by long-term field experiments (experimental treatments, temporal data, environmental covariates, soil productivity, soil management),</li> <li>filter by record's type (dataset, document, publication, software, services, series).</li> </ul>"},{"location":"technical_components/catalogue/#future-work","title":"Future work","text":"<ul> <li>extend fulltext search; allow complex queries using exact match, OR,...</li> <li>use Full Text Search ranking to sort by relevance.</li> <li>filter by source repository.</li> </ul>"},{"location":"technical_components/catalogue/#display-records-detail","title":"Display record's detail","text":"<p>After clicking result's table item, a record's detail is displayed at unique URL address to facilitate sharing. Record's detail currently comprises:</p> <ul> <li>record's type tag,</li> <li>full title,</li> <li>full abstract,</li> <li>keywords' tags,</li> <li>preview of record's geographical extent, see Map preview,</li> <li>record's preview image, if available,</li> <li>all other record's items,</li> <li>section enabling User Engagement,</li> <li>last update date.</li> </ul>"},{"location":"technical_components/catalogue/#future-work_1","title":"Future work","text":"<ul> <li>links section with links to original repository, TBD...,</li> <li>indication of metadata augmentation, such as link liveliness assessment,</li> <li>display metadata augmentation results,</li> <li>display metadata validation results,</li> <li>show relations to other records,</li> <li>better distinguish link types; service/api, download, records, documentation, etc.</li> </ul>"},{"location":"technical_components/catalogue/#resource-preview","title":"Resource preview","text":"<p>SoilWise Catalogue currently supports 3 types of preview:</p> <ul> <li>Display resource geographical extent, which is available in the record's detail, as well in the search results list.</li> <li>Display of a graphic preview (thumbnail) in case it is advertised in metadata.</li> <li>Map preview of OGC:WMS services advertised in metadata enables standard simple user interaction (zoom, changing layers).</li> </ul>"},{"location":"technical_components/catalogue/#data-download-as-is","title":"Data download (AS IS)","text":"<p>Download of data \"as is\" is currently supported through the links section from the harvested repository. Note, \"interoperable data download\" has been only a proof-of-concept in the first iteration phase, i.e. is not integrated into the SoilWise Catalogue.</p>"},{"location":"technical_components/catalogue/#display-link-to-knowledge","title":"Display link to knowledge","text":"<p>Download of knowledge source \"as is\" is currently supported through the links section from the harvested repository.</p>"},{"location":"technical_components/catalogue/#support-catalogue-apis-of-various-communities","title":"Support catalogue API's of various communities","text":"<p>In order to interact with the many relevant data communities, Soilwise aims to support a range of catalogue standards.</p>"},{"location":"technical_components/catalogue/#catalogue-service-for-the-web","title":"Catalogue Service for the Web","text":"<p>Catalogue service for the web (CSW) is a standardised pattern to interact with (spatial) catalogues, maintained by OGC. </p>"},{"location":"technical_components/catalogue/#ogc-api-records","title":"OGC API - Records","text":"<p>OGC is currently in the process of adopting a revised edition of its catalogue standards. The new standard is called OGC API - Records. OGC API - Records is closely related to Spatio Temporal Asset Catalogue (STAC), a community standard in the Earth Observation community. </p>"},{"location":"technical_components/catalogue/#protocol-for-metadata-harvesting","title":"Protocol for metadata harvesting","text":"<p>The open archives initiative has defined a common protocol for metadata harvesting (oai-pmh), which is adopted by many catalogue solutions, such as Zenodo, OpenAire, CKAN. The oai-pmh endpoint of Soilwise can be harvested by these repositories.</p>"},{"location":"technical_components/catalogue/#schemaorg-annotiations","title":"Schema.org annotiations","text":"<p>Annotiations using schema.org/Dataset ontology enable search engines to harvest metadata in a structured way.</p>"},{"location":"technical_components/catalogue/#user-engagement","title":"User Engagement","text":"<p>Collecting users feedback provides an important channel on the usability of described resources. Users can even support each other by sharing the feedback as 'questions and answers'. For this purpose every display of a record is concluded with a feedback section where users can interact about the resource. Users need to authenticate to provide feedback.</p>"},{"location":"technical_components/catalogue/#future-work_2","title":"Future work","text":"<p>Notify the resource owners of incoming feedback, so they can answer any questions or even improve their resource.</p>"},{"location":"technical_components/catalogue/#technology","title":"Technology","text":"<p>pycsw  is a catalogue component offering an HTML frontend and query interface using various standardised catalogue APIs to serve multiple communities. Pycsw, written in python, allows for the publishing and discovery of geospatial metadata via numerous APIs (CSW 2/CSW 3, OpenSearch, OAI-PMH, SRU), providing a standards-based metadata and catalogue component of spatial data infrastructures. pycsw is Open Source, released under an MIT license, and runs on all major platforms (Windows, Linux, Mac OS X).</p> <p>pycsw is deployed as a docker container from the official docker hub repository. Its configuration is updated at deployment. Some layout templates are overwritten at deployment to facilitate a tailored HTML view.</p>"},{"location":"technical_components/catalogue/#integration","title":"Integration","text":"<p>The SWR catalogue component will show its full potential when integrated to (1) Harvester, (2) Storage of metadata,  (3) Metadata Augmentation and Metadata Validation. </p>"},{"location":"technical_components/ingestion/","title":"Harvester","text":"<p>Info</p> <p>Current version: 0.1.0</p> <p>Project: Harvesters</p> <p>The Harvester component is dedicated to automatically harvest sources to populate SWR with metadata on datasets and knowledge sources.</p>"},{"location":"technical_components/ingestion/#metadata-harvesting-concept","title":"Metadata harvesting concept","text":"<p>Metadata harvesting is the process of ingesting metadata, i.e. evidence on data and knowledge, from remote sources and storing it locally in the catalogue for fast searching. It is a scheduled process, so local copy and remote metadata are kept aligned. Various components exist which are able to harvest metadata from various (standardised) API's. SoilWise aims to use existing components where available.</p> <p>The harvesting mechanism relies on the concept of a universally unique identifier (UUID) or unique resource identifier (URI) that is being assigned commonly by metadata creator or publisher. Another important concept behind the harvesting is the last change date. Every time a metadata record is changed, the last change date is updated. Just storing this parameter and comparing it with a new one allows any system to find out if the metadata record has been modified since last update. An exception is if metadata is removed remotely. SoilWise Repository can only derive that fact by harvesting the full remote content. Discussion is needed to understand if SWR should keep a copy of the remote source anyway, for archiving purposes. All metadata with an update date newer then last-identified successfull harvester run are extracted from remote location. </p> <p>A harvesting task typically extracts records with update-date later then the last-identified successfull harvester run.</p> <p>Harvested content is (by default) not editable for the following reasons:</p> <ol> <li>The harvesting is periodic so any local change to harvested metadata will be lost during the next run.</li> <li>The change date may be used to keep track of changes so if the metadata gets changed, the harvesting mechanism may be compromised.</li> </ol> <p>If inconsistencies with imported metadata are identified, we can add a statement to the graph of such inconsistencies. We can also notify the author of the inconsistency so they can fix the inconsistency on their side.</p> <p>A governance aspect still under discussion is if harvested content is removed as soon as a harvester configuration is removed, or when records are removed from the remote endpoint. The risk of removing content is that relations within the graph are breached. An alternative is to indicate the record has been archived by the provider.</p> <p>Typical tasks of a harvester:</p> <ul> <li>Define a harvester job<ul> <li>Schedule (on request, weekly, daily, hourly)</li> <li>Endpoint / Endpoint type (example.com/csw -&gt; OGC:CSW)</li> <li>Apply a filter (only records with keyword='soil-mission')</li> </ul> </li> <li>Understand success of a harvest job <ul> <li>overview of harvested content (120 records)</li> <li>which runs failed, why? (today failed -&gt; log, yesterday successfull -&gt; log)</li> <li>Monitor running harvestors (20% done -&gt; cancel)</li> </ul> </li> <li>Define behaviours on harvested content<ul> <li>skip records with low quality (if test xxx fails)</li> <li>mint identifier if missing ( https://example.com/data/{uuid} )</li> <li>a model transformation before ingestion ( example-transform.xsl / do-something.py )</li> </ul> </li> </ul>"},{"location":"technical_components/ingestion/#resource-types","title":"Resource Types","text":"<p>Metadata for following resource types are foreseen to be harvested:</p> <ul> <li>Data &amp; Knowledge Resources </li> <li>Organisations, Projects, LTE, Living labs initiatives</li> <li>Repositories/Catalogues</li> </ul> <p>These entities relate to each other as:</p> <pre><code>flowchart LR\n    people --&gt;|memberOf| o[organisations] \n    o --&gt;|partnerIn| p[projects]\n    p --&gt;|produce| d[data &amp; knowledge resources]\n    o --&gt;|publish| d\n    d --&gt;|describedIn| c[catalogues]\n    p --&gt;|part-of| fs[Fundingscheme]\n</code></pre>"},{"location":"technical_components/ingestion/#datasets","title":"Datasets","text":"<p>Metadata records of datasets are, for the first iteration, primarily imported from the ESDAC, INSPIRE GeoPortal, BonaRes and Cordis/OpenAire. In later iterations SoilWise aims to include other projects and portals, such as national or thematic portals. These repositories contain large number of datasets. Selection of key datasets concerning the SoilWise scope is a subject of know-how to be developed within SoilWise.</p>"},{"location":"technical_components/ingestion/#knowledge-sources","title":"Knowledge sources","text":"<p>With respect to harvesting, it is important to note that knowledge assets are heterogeneous, and that (compared to data), metadata standards and particularly access / harvesting protocols are not generally adopted. Available metadata might be implemented using a proprietary schema, and basic assumptions for harvesting, e.g. providing a \"date of last change\" might not be offered. This will, in some cases, make it necessary to develop customized harvesting and metadata extraction processes. It also means that informed decisions need to be made on which resources to include, based on priority, required efforts and available capacity.</p> <p>The SoilWise project team is still exploring which knowledge resources to include. As an example, an important cluster of knowledge sources may be seen academic articles and report deliverables from Mission Soil Horizon Europe projects. These resources are accessible from ESDAC, Cordis and OpenAire. Extracting content from Cordis, OpenAire can be achieved using a harvesting task (using the Cordis schema, extended with post processing). For the first iteration, SoilWise aims to achieve this goal. In future iterations new knowledge sources may become relevant, we will investigate at that moment what is the best approach to harvest them.</p>"},{"location":"technical_components/ingestion/#functionality","title":"Functionality","text":"<p>The Harvester component currently comprises of the following functions:</p> <ul> <li>Harvest records from metadata and knowledge resources</li> <li>Metadata harmonization</li> <li>Metadata RDF turtle serialization</li> <li>RDF to Triple Store</li> <li>Duplication identification</li> </ul>"},{"location":"technical_components/ingestion/#harvest-records-from-metadata-and-knowledge-resources","title":"Harvest records from metadata and knowledge resources","text":"<p>Note, the first SoilWise Repository development iteration resulted in 9,0444 harvested metadata records (to date 12.09.20241).</p>"},{"location":"technical_components/ingestion/#cordis","title":"CORDIS","text":"<p>European Research projects typically advertise their research outputs via Cordis. This makes Cordis a likely candidate to discover research outputs, such as reports, articles and datasets. Cordis does not capture many metadata properties. In those cases where a resource is identified by a DOI, additional metadata can be found in OpenAire via the DOI. The scope of projects, from which to include project deliverables is still under discussion. </p> <p>Which projects to include is derived from 2 sources:</p> <ul> <li>ESDAC maintains a list of historic EU funded research projects</li> <li>Mission soil platform maintains a list of current Mission soil projects</li> </ul> <p>A script fetches the content from these 2 sources and prepares for the CORDIS and OpenAire harvested to understand which content is relevant. The content in these pages is unstructured html. The content is scraped using a python library. This is not optimal, because the scraper expects a dedicated html structure, which is fragile.</p> <p>Results of the scrape activity are stored in table <code>harvest.projects</code>. For each project a Record control number(RCN) is retrieved from the Cordis knowledge graph. This RCN could be used to filter OpenAire, however OpenAire can also be filtered using project grant number. At this moment in time the Cordis Knowledge graph does not contain the Mission Soil projects yet. </p> <p>At this moment in time we do not harvest resources from Cordis which do not have a DOI. This includes mainly progress reports of the projects. </p>"},{"location":"technical_components/ingestion/#openaire","title":"OpenAire","text":"<p>For those resources, discovered via Cordis/ESDAC, and identified by a DOI, a harvester fetches additional metadata from OpenAire. OpenAire is a catalogue initiative which harvests metadata from popular scientific repositories, such as Zenodo, Dataverse, etc.</p> <p>Not all DOI's registered in Cordis are available in OpenAire. OpenAire only lists resources with an open access license. Other DOI's can be fetched from the DOI registry directly or via Crossref.org. This work is still in preparation.</p> <p>Records in OpenAire are stored in the Open Aire Research Graph (OAF) format, which is transformed to a metadata set based on Dublin Core.</p>"},{"location":"technical_components/ingestion/#ogc-csw","title":"OGC-CSW","text":"<p>Many (spatial) catalogues advertise their metadata via the catalogue Service for the Web standard, such as INSPIRE GeoPortal, Bonares, ISRIC. The OWSLib library is used to query records from CSW endpoints. A filter can be configured to retrieve subsets of the catalogue.</p> <p>Incidentally, records advertised as CSW also include a DOI reference (Bonares/ISRIC). Additional metadata for these DOI's is extracted from OpenAire/Crossref.</p>"},{"location":"technical_components/ingestion/#inspire","title":"INSPIRE","text":"<p>Although INSPIRE Geoportal does offer a CSW endpoint, due to a technical reasons, we have not been able to harvest from it. Instead we have developed a dedicated harvester via the Elastic Search API endpoint of the Geoportal. If at some point the technical issue has been resolved, use of the CSW harvest endpoint is favourable.</p>"},{"location":"technical_components/ingestion/#esdac","title":"ESDAC","text":"<p>The ESDAC catalogue is an instance of Drupal CMS. We have developed a dedicated harvester to scrape html elements to extract Dublin Core metadata from ESDAC html elements. Metadata is extracted for datasets, maps (EUDASM) and documents. Incidentally a DOI is mentioned as part of the HTML, this DOI is then used as identifier for the resource, else the resource url is used as identifier. If the DOI is not known to the system yet, OpenAire will be queried to capture additional metadata on the resource.</p>"},{"location":"technical_components/ingestion/#impact4soil","title":"Impact4Soil","text":"<p>Impact4soil is build on a Strapi.io headless CMS. The CMS provides an API to retrieve datasets and scientific articles. The API provides minimal metadata, but fortunately in most cases a DOI is included. DOI is used to capture additional metadata from OpenAire.</p>"},{"location":"technical_components/ingestion/#prepsoil-portal","title":"Prepsoil portal","text":"<p>Prep4soil is build on a headless CMS. The CMS at times provides an API to retrieve datasets, knowledge items, living labs, lighthouses and communities of practice. The API provides minimal metadata, incidentally a DOI is included. DOI is used to capture additional metadata from OpenAire.</p>"},{"location":"technical_components/ingestion/#metadata-harmonization","title":"Metadata Harmonization","text":"<p>Once stored in the harvest sources database, a second process is triggered which harmonizes the sources to the desired metadata profile. These processes are split by design, to prevent that any failure in metadata processing would require to fetch remote content again.</p> <p>Table below indicates the various source models supported</p> source platform Dublin Core Cordis Extended Dublin core ESDAC Datacite OpenAire, Zenodo, DOI ISO19115:2005 Bonares, INSPIRE <p>Metadata is harmonised to a DCAT RDF representation.</p> <p>For metadata harmonization some supporting modules are used, owslib is a module to parse various source metadata models, including iso19139:2007. A transformation script from (semic-eu/iso19139-to-dcat-ap.xslt)[https://github.com/semic-eu/iso19139-to-dcat-ap/] in combination with lxml and rdflib is used to convert iso19139:2007 metadata to RDF, serialised as turtle.</p> <p>Harmonised metadata is either transformed to iso19139:2007 or Dublin Core and then ingested by the pycsw software, used to power the SoilWise Catalogue, using an automated process running at intervals. At this moment the pycsw catalogue software requires a dedicated database structure. This step converts the harmonised metadata database to that model. In next iterations we aim to remove this step and enable the catalogue to query the harmnised model directly.</p>"},{"location":"technical_components/ingestion/#metadata-augmentation","title":"Metadata Augmentation","text":"<p>The metadata augmentation processes are described elsewhere, what is relevant here is that the output of these processes is integrated in the harmonised metadata database.</p>"},{"location":"technical_components/ingestion/#metadata-rdf-turtle-serialization","title":"Metadata RDF turtle serialization","text":"<p>The harmonised metadata model is based on the DCAT ontology. In this step the content of the database is written to RDF.</p> <p>Harmonized metadata is transformed to RDF in preparation of being loaded into the triple store (see also Knowledge Graph).</p>"},{"location":"technical_components/ingestion/#rdf-to-triple-store","title":"RDF to Triple store","text":"<p>This is a component which on request can dump the content of the harmonised database as an RDF quad store. This service is requested at intervals by the triple store component. In a next iteration we aim to push the content to the triple store at intervals.</p>"},{"location":"technical_components/ingestion/#duplication-indentification","title":"Duplication indentification","text":"<p>A resource can be described in multiple Catalogues, identified by a common identifier. Each of the harvested instances may contain duplicate, alternative or conflicting statements about the resource. SoilWise Repository aims to persist a copy of the harvested content (also to identify if the remote source has changed). For this iteration we store the first copy, and capture on what other platforms the record has been discovered. OpenAire already has a mechanism to indicate in which platforms a record has been discovered, this information is ingested as part of the harvest. An aim of this exercise is also to understand in which repositories a certain resource is advertised.</p> <p>Visualization of source repositories is in the first development iteration available as a dedicated section in the SoilWise Catalogue.</p>"},{"location":"technical_components/ingestion/#technology","title":"Technology","text":""},{"location":"technical_components/ingestion/#git-actionspipelines-to-run-harvest-tasks","title":"Git actions/pipelines to run harvest tasks","text":"<p>Git actions (github) or pipelines (gitlab) are automated processes which run at intervals or events. Git platforms typically offer this functionality including extended logging, queueing, and manual job monitoring and interaction (start/stop).</p> <p>Each harvester runs in a dedicated container. The result of the harvester is ingested into a (temporary) storage. Follow up processes (harmonization, augmentation, validation) pick up the results from the temporary storage. </p> <p><pre><code>flowchart LR\n    c[CI-CD] --&gt;|task| q[/Queue\\]\n    r[Runner] --&gt; q\n    r --&gt;|deploys| hc[Harvest container]\n    hc --&gt;|harvests| db[(temporary storage)]\n    hc --&gt;|data cleaning| db[(temporary storage)]\n</code></pre> Harvester tasks are triggered from Git CI-CD, Git provides options to cancel and trigger tasks and review CI-CD logs to check errors</p>"},{"location":"technical_components/ingestion/#ogc-csw_1","title":"OGC-CSW","text":"<p>Many (spatial) catalogues advertise their metadata via the catalogue Service for the Web standard, such as INSPIRE GeoPortal, Bonares, ISRIC.</p>"},{"location":"technical_components/ingestion/#cordis-openaire","title":"CORDIS - OpenAire","text":"<p>Cordis does not capture many metadata properties. We harvest the title of a project publication and, if available, the DOI. In those cases where a resource is identified by a DOI, additional metadata can be found in OpenAire via the DOI. For those resources a harvester fetches additional metadata from OpenAire. </p> <p>A second mechanism is available to link from Cordis to OpenAire, the RCN number. The OpenAire catalogue can be queried using an RCN filter to retrieve only resources relevant to a project. This work is still in preparation.</p> <p>Not all DOI's registered in Cordis are available in OpenAire. OpenAire only lists resources with an open access license. Other DOI's can be fetched from the DOI registry directly or via Crossref.org. This work is still in preparation. Detailed technical information can be found in the technical description.</p>"},{"location":"technical_components/ingestion/#openaire-and-other-sources","title":"OpenAire and other sources","text":"<p>The software used to query OpenAire by DOI or by RCN is not limited to be used by DOIs or RCNs that come from Cordis. Any list of DOIs or list of RCNs can be handled by the software.</p>"},{"location":"technical_components/ingestion/#integration-opportunities","title":"Integration opportunities","text":"<p>The Automatic metadata harvesting component will show its full potential when being in the SWR tightly connected to (1) SWR Catalogue, (2) Metadata authoring and (3) ETS/ATS, i.e. test suites.</p>"},{"location":"technical_components/interlinker/","title":"Interlinker","text":"<p>Info</p> <p>Current version: v0.1</p> <p>Harvesting metadata from data and knowledge assets from a multitude of repositories will result in a large and diverse collection of metadata. Some of these assets will be related, which will sometimes be explicitly expressed in the (meta)data. In many cases, such relationships are implicit and can only be discovered by further analysing the (meta)data and content. The interlinker component provides the functions to find such relationships and add them as links to the SoilWise knowledge graph.</p> <p>Examples are:</p> <ul> <li>Assets that direclty refer to other assets as part of their metadata (e.g. a dataset refering to a document describing the data collection and processing)</li> <li>Assets that indirectly refer to other assets (e.g. a dataset being mentioned in the text of a document)</li> <li>Assets with the same DOI and metadata coming from different repositories</li> <li>Assets with the same DOI, but different metadata fields</li> <li>Assets without DOI that have identical metadata</li> <li>Assets without DOI that have similar, but not identical metadata</li> </ul> <p>Interlinker component comprises of the following functions:</p> <ol> <li>Automatic metadata interlinking</li> <li>Similarity Finder (formerly Duplicates identification)</li> <li>Link liveliness assessment</li> </ol>"},{"location":"technical_components/interlinker/#automatic-metadata-interlinking","title":"Automatic metadata interlinking","text":"<p>Info</p> <p>Current version: v0.1</p> <p>Projects: Automatic metadata interlinking (missing repo)</p> <p>To be able to provide interlinked data and knowledge assets (e.g. a dataset, the project in which it was generated and the operating procedure used) links between metadata must be identified and registered ideally as part of the SWR Triple Store.</p> <p>We distinguish between explicit and implicit links:</p> <ul> <li>Explicit links can be directly derived from the data and/or metadata. E.g. projects in CORDIS are explicitly linked to documents and datasets. </li> <li>Implicit links can not be directly derived from the (meta)data. They may be derived by spatial or temporal extent, keyword usage, or shared author/publisher. </li> </ul> <p>SWR-1 implements the interlinking of data and knowledge assets based on explicit links that are found in the harvested metadata. The harvesting processes implemented in SWR-1 have been extended with this function to detect such linkages and store them in the repository and add them to the SWR knowledge graph. This allows e.g. exposing this additional information to the UI for displaying and linkage the  and other functions. </p> <p>!! @Paul, @Hugo to add relevant details !! Do we link to the harvester component for now?</p>"},{"location":"technical_components/interlinker/#similarity-finder","title":"Similarity Finder","text":"<p>Info</p> <p>Current version:</p> <p>Projects: Similarity finder</p> <p>The Similarity Finder identifies similarities over different data and knowledge assets by implementing functionality to compare their metadata. This reveals information on duplicities (the assets are the same) or similarities (the assets are similar with regard to specific aspects) and captures it as part of the knowledge graph.</p> <p>In SWR-1, this subcomponent implements the functionality to detect duplicates. This is performed based on the comparision of Persistent Identifiers (PIDs) as well as on the comparison of a set of key metadata elements (to detect situations where multiple identifiers point to the same digital object or resource). The process is currently performed at metadata harvesting, but can also be run independently for quality checks.</p> <p>!! @Paul, @Hugo to add relevant details</p>"},{"location":"technical_components/interlinker/#link-liveliness-assessment","title":"Link liveliness assessment","text":"<p>Info</p> <p>Current version:</p> <p>Projects: Link liveliness assessment</p> <p>Metadata (and data and knowledge sources) tend to contain links to other resources. Not all of these URIs are persistent, so over time they can degrade. In practice, many non-persistent knowledge sources and assets exist that could be relevant for SWR, e.g. on project websites, in online databases, on the computers of researchers, etc. Links pointing to such assets might however be part of harvested metadata records or data and content that is stored in the SWR. </p> <p>The link liveliness assessment subcomponent runs over the available links stored with the SWR assets and checks their status. The function is foreseen to run frequently over the URIs in the SWR repository, assessing and storing the status of the link. The link liveliness checker privides:</p> <ul> <li>up to date health status of every assessed link</li> <li>health status history</li> <li>analytics functions, e.g. aggregating health status history to generate health indicators for UI visualisation. </li> </ul> <p>!! @Vassilis to add relevant details !! Proposal from @Rob to move this component to metadata augmentation</p>"},{"location":"technical_components/interlinker/#the-old-content","title":"The Old Content","text":"<p>Interlinker component comprises of the following functions:</p> <ol> <li>Automatic metadata interlinking</li> <li>Similarity Finder (formerly Duplicates identification)</li> <li>Link liveliness assessment</li> </ol>"},{"location":"technical_components/interlinker/#automatic-metadata-interlinking_1","title":"Automatic metadata interlinking","text":"<p>To be able to provide interlinked data and knowledge assets (e.g. a dataset, the project in which it was generated and the operating procedure used) links between metadata must be identified and registered ideally as part of the SWR Triple Store.</p> <ul> <li>Explicit links can be directly derived from the data and/or metadata. E.g. projects in CORDIS are explicitly linked to documents and datasets.  For those linkages, the harvesting process needs to be extended, calling this component to store the relation in the knowledge graph. It should accommodate \"vice versa linkage\" (if resource A links to B, a vice versa link can be added to B).</li> <li>Implicit links can not be directly derived from the (meta)data. They may be derived by spatial or temporal extent, keyword usage, or shared author/publisher. In this case, AI/ML can support the discovery of potential links, including some kind of probability indicator.</li> </ul>"},{"location":"technical_components/interlinker/#duplicates-identification","title":"Duplicates identification","text":"<p>In the context of Persistent Identifiers (PIDs), duplication refers to the occurrence of multiple identifiers pointing to the same digital object or resource. As SWR will be ingesting datafiles from multiple data sources, this is an aspect that has to be taken into account. </p> <p>We have no knowledge of existing technologies we can integrate as a component of the platform. This functionality will be setup within the platform.  The methodology applied to identify duplicates will be by comparing multiple (meta)data attributes like File Name, File Size, File Type, Owner, Description, Date Created/Modified.  Natural Language Processing techniques like Bag-of-words or Word/Sentence Embedding algorithms can be used to convert textual attributes into vectors, capturing semantic similarity and relationships between words. Each datafile will be characterized by its attributes and represented in a continuous vector space together with the other datafiles. Similarity algorithms (e.g. cosine similarity, euclidean distance, etc.) are then applied to identify datafiles with a similarity above a certain threshold, which is then considered to be duplicated. If necessary, a business rule will be integrated, taking the \"completeness\" of the datafile into account as to be able to determine which PID and datafile to keep and which to discard.</p>"},{"location":"technical_components/interlinker/#technology","title":"Technology","text":"<p>This process can be automated in the platform using automated (Python) scripts running within the platform's data processing environment. A second approach is to use data processing functionalities and AI algorithms integrated into a database, e.g. the Neo4J Graph Database and Neo4J Graph Data Science Similarity algorithms  (Node Similarity, K-Nearest Neighbours, ...). This requires the data to exist in the graph database as linked data, either importing from the SWR knowledge graphs or using such a graph database technology (e.g. Neo4J) as the SWR knowledge graph technology.</p> <ul> <li>two levels inspection (coarse = dataset level, fine = objects/attributes? level)</li> <li>read existing data in terms of size, identical identifiers (data, metadata level)</li> <li>identify duplicite values</li> </ul>"},{"location":"technical_components/interlinker/#link-liveliness-assessment_1","title":"Link liveliness assessment","text":"<p>Info</p> <p>Current version:</p> <p>Projects: Link liveliness assessment</p> <p>Persistent content is considered to be stored in a trustworthy, persistent repository. We expect those storages to store the asset compliant with the applicable legally and scientifically required terms and periods for storage of the content, and to use a DOI or other persistent URI for persistent identification. These can be safely referred to from the SoilWise catalogue. For long-term preservation and availability of data and knowledge assets, SWR relies on the repository holders and their responsibility to keep it available.</p> <p>Non-persistent data and knowledge are the ones that are not guaranteed to persist by the repository or data and knowledge holder and/or do not guarantee a persistent URI for reference for at least 10 years. In practice, many non-persistent knowledge sources and assets exist that could be relevant for SWR, e.g. on project websites, in online databases, on the computers of researchers, etc. Due to their heterogeneity in structure and underlying implementing technologies, etc., it is not possible nor desirable to store those in the SWR, with the exception of high-value data/knowledge assets.  </p>"},{"location":"technical_components/interlinker/#foreseen-functionality","title":"Foreseen functionality","text":"<ul> <li>Assess if resources use proper identifiers to reference external items. Metadata (and data and knowledge sources) tend to contain links which, over time, degrade and result in <code>File not found</code> experiences. </li> <li>By running availability checks on links mentioned in (meta)data, for each link an availability indicator (available, requires authentication, intermittent, unavailable) can be calculated. </li> <li>Alternatively, an availability check can be performed at the moment a user tries to open a resource.</li> </ul>"},{"location":"technical_components/interlinker/#technology_1","title":"Technology","text":"<p>Providers of Identifiers:</p> <ul> <li>ePIC  ePIC API providing a software stack for a PID service</li> <li>DOI </li> <li>w3id.org persistent identification at namespace/domain level</li> <li>R3gistry Germany for namespaces, codelists, identifiers. Similar exist for Austria, Italy, Spain, Slovakia, Netherlands</li> </ul> <p>Liveliness checks:</p> <ul> <li>GeoHealthCheck a library which checks at intervals the availability of OGC APIs up to collection/item level, should be extended to drill down from CSW endpoint to record level and check links in individual records </li> <li>Geocat has developed a linkage checker for iso19139:2007 metadata for INSPIRE geoportal, available at icat, which includes link checks in OWS capabilities.</li> <li>Python link checker checks (broken) links in html</li> <li>...</li> </ul>"},{"location":"technical_components/interlinker/#similarity-finder_1","title":"Similarity finder","text":"<p>Info</p> <p>Current version:</p> <p>Projects: Similarity finder</p>"},{"location":"technical_components/knowledge_graph/","title":"Knowledge Graph","text":"<p>Info</p> <p>Current version: 0.1.0 </p> <p>Project: Soil Health Knowledge graph</p> <p>Access point: SWR SPARQL endpoint: https://sparql.soilwise-he.containers.wur.nl/sparql</p> <p>SoilWise develops and implements a Knowledge Graph linking the knowledge captured in harvested and augmented metadata with various sources of internal and external knowledge sources, particularly taxonomies, vocabularies and ontologies that are also implemented as RDF graphs. Linking such graphs into a harmonized SWR Knowledge Graph allows reasoning over the relations in the stored graph, and thus allows connecting and smartly combining knowledge from those domains.</p> <p>The first iteration of the SWR Knowledge Graph is a graph representation of the (harmonized) metadata that is currently harvested, validated and augmented as part of the SWR catalogue database. It's RDF representation, stored in a triple store, and the SPARQL endpoint deployed on top of the triple store, allow users alternate access to the metadata, exploiting semantics and relations between different assets. </p> <p>At the same time, experiments have been performed to prepare for the linkage of this RDF metadata graph and existing and AI/ML generated graphs. In future iterations, the metadata graph will be linked/merged with a dedicated soil health knowledge graph also linking to external resources, establishing a broader interconnected soil health knowledge graph. Consequently, it will evolve into a knowledge network that allows much more powerful and impactful queries and reasoning, e.g. supporting decision support and natural language quering.</p>"},{"location":"technical_components/knowledge_graph/#functionality","title":"Functionality","text":""},{"location":"technical_components/knowledge_graph/#knowledge-graph-querying-sparql-endpoint","title":"Knowledge Graph querying (SPARQL endpoint)","text":"<p>The SPARQL endpoint, deployed on top of the SWR triple store, allows end users to query the SWR knowledge graph using the SPARQL query language. It is the primary access point to the knowledge graph, both for humans, as well as for machines. Many applications and end users will instead interact with specialised assets that use the SPARQL end-point, such as the Chatbot or the API. However, the SPARQL end-point is the main source for the development of further knowledge applications and provides bespoke search to humans.</p> <p>Since we're importing resources from various data and knowledge repositories, we expect many duplicities, blank nodes and conflicting statements. Implementation of rules should be permissive, not preventing inclusion, only flag potential inconsistencies.</p>"},{"location":"technical_components/knowledge_graph/#ongoing-developments","title":"Ongoing Developments","text":""},{"location":"technical_components/knowledge_graph/#knowledge-graph-enrichment-and-linking","title":"Knowledge Graph enrichment and linking","text":"<p>Info</p> <p>Access point: https://voc.soilwise-he.containers.wur.nl/concept/</p> <p>As a preparation to extend the currently deployed metadata knowledge graph (KG) with broader domain knowledge, experimental work has been performed to enrich the KG to link it with other knowledge graphs. </p> <p>The following aspects have been worked on and will  be furhter developed and integrated into future iterations of the SoilWise KG:</p> <ul> <li>Applying various methods using AI/ML to derive a (soil health) knowledge graph from unstructured content. This is piloted by using (parts of) the EEA report \"Soil monitoring in Europe - Indicators and thresholds for soil quality assessments\". It tests the effectiveness of various methods to generate knowledge in the form of KGs from documents, which could also benefit other AI/ML functions foreseen.</li> <li>Establishing links between the SoilWise KG and external taxonomies and ontologies (linked data). Concepts in the SoilWise KG that (closely) match with concepts in the AGROVOC thesaurus are linked. The implemented method is exemplary for the foreseen wider linking required to establish a soil health KG.</li> <li>Testing AI/ML based methods to derive additional knowledge (e.g. keywords, geography) for data and knowledge assets. Such methods could for instance be used to further augment metadata or fill exisiting metadata gaps. Besides testing such methods, this includes establishing a model that allows to distinguish between genuine and generated metadata.</li> </ul>"},{"location":"technical_components/knowledge_graph/#technology-integration","title":"Technology &amp; Integration","text":"<p>Components used:</p> <ul> <li>Virtuoso (version 07.20.3239)</li> <li>Python notebooks</li> </ul> <p>Ontologies/Vocabularies/Schemas:</p> <ul> <li>SKOS Core</li> <li>Dublin Core</li> <li>AGROVOC</li> <li>GloSIS</li> <li>Agrontology</li> <li>QUDT</li> </ul>"},{"location":"technical_components/mapserver/","title":"Map Server","text":"<p>Info</p> <p>Current version: 8.2.2</p> <p>Project: MapServer</p>"},{"location":"technical_components/mapserver/#functionality","title":"Functionality","text":"<p>MapServer is an Open Source platform for publishing spatial data to the web with standardised APIs defined by Open Geospatial Consortium, such as WMS, WFS, WCS, OGC API-Features. Originally developed in the mid-1990s at the University of Minnesota, MapServer is released under an MIT-style license and runs on all major platforms (Windows, Linux, Mac OS X). MapServer is not a full-featured GIS system, nor does it aspire to be. </p> <p>In Soilwise mapserver will be used to share spatial data in a conveniant way to clients such as QGIS, ArcGIS, OpenLayers, Leaflet.</p> <p>Read more about MapServer at EJPSoil wiki.</p>"},{"location":"technical_components/mapserver/#technology","title":"Technology","text":"<p>A docker image for mapserver is maintained by Camp2Camp. The important aspect here is that the image uses a minimal build of GDAL, which defines the source formats consumable by the MapServer (in line with section Transformation and Harmonistation Components). If formats such as Geoparquet or Geozarr are relevant, building a tailored image is relevant.</p> <p>The configuration of the MapServer is managed via a config file. The config files reference metadata, data and styling rules. Various tools exist to create MapServer config files:</p> <ul> <li>geocat bridge is a QGIS plugin to create mapfiles from QGis projects</li> <li>Mappyfile is a python library to generate mapfiles by code</li> <li>mapserver studio a saas solution to edit mapfiles</li> <li>mapscript is a python library to interact with the MapServer binary </li> <li>pygeodatacrawler is a tool by ISRIC generating mapfiles from various resources</li> <li>vs code mapfile plugin</li> </ul> <p>Mapfiles produced in the project are maintained in a git repository and mounted into the mapserver container in a deployment phase.</p>"},{"location":"technical_components/mapviewer/","title":"Map viewer","text":"<p>Info</p> <p>Current version:</p> <p>Project: </p>"},{"location":"technical_components/mapviewer/#functionality","title":"Functionality","text":"<p>A light-weight client map viewer component will be employed:</p> <ul> <li>as a frontend of Map Server component to visualize provided WMS, WFS, WCS layers, but also external services.</li> <li>as an integrated part of the Catalogue to visualize primarily the geographical extent of data described in the metadata record and a snapshot visualization of the data</li> <li>to provide the full preview of data, when a link to web service or data browse graphics (preview image) is available</li> </ul> <p>A dedicated mapviewer, can support spatial oriented users in accessing relevant data in a spatial way. For example maps of spatial distribution of soil properties or health indicators over Europe. A typical example is Soilgrids.</p> <p>An interesting aspect of a community like EUSO is the ability to prepare and share a map with stakeholders to trigger some discussion on phenomena at a location.</p> <p>Examine the need for viewing novel formats such as Vector tiles, COG, GeoZarr, GeoParquet directly on a map background. The benefit of these formats is that no (OGC) API is required to facilitate data visualisation.</p>"},{"location":"technical_components/mapviewer/#technology","title":"Technology","text":"<p>The mapviewer in the catalogue is the leaflet javascript library on a backdrop of OpenStreetMap tiles (provided by the OpenStreetMap community).</p> <p>If a more elaborate mapviewer is requested, a suggestion could be the terriajs library. TerriaJS is an environment to share maps (react+leaflet+cesium), but also create maps and share them with stakeholders.</p>"},{"location":"technical_components/metadata_augmentation/","title":"Metadata Augmentation","text":"<p>Info</p> <p>Current version: 0.1.0</p> <p>Project: Metadata augmentation</p>"},{"location":"technical_components/metadata_augmentation/#functionality","title":"Functionality","text":"<p>In this component scripting / NLP / LLM are used on a metadata record to augment metadata statements about the resource. Augmentations are stored on a dedicated augmentation table, indicating the process which produced it.</p> metadata-uri metadata-element source value proces date https://geo.fi/data/ee44-aa22-33 spatial-scope 16.7,62.2,18,81.5 https://inspire.ec.europa.eu/metadata-codelist/SpatialScope/national spatial-scope-analyser 2024-07-04 https://geo.fi/data/abc1-ba27-67 soil-thread This dataset is used to evaluate Soil Compaction in Nuohous Sundstr\u00f6m http://aims.fao.org/aos/agrovoc/c_7163 keyword-analyser 2024-06-28 <p>For the first SoilWise prototype, the functionality of the Metadata Augmentation component comprises:</p> <ul> <li>Automatic metadata generation</li> <li>Translation module</li> </ul>"},{"location":"technical_components/metadata_augmentation/#automatic-metadata-generation","title":"Automatic metadata generation","text":"<p>To generate metadata (data set and service metadata), activate the corresponding button(s) when setting up the theme for the transformation process. The steps are described here</p>"},{"location":"technical_components/metadata_augmentation/#translation-module","title":"Translation module","text":"<p>Many records arrive in a local language, SWR translates the main properties for the record: title and abstract into English, to offer a single language user experience. The translations are used in filtering and display of records.</p> <p>The translation module builds on the EU translation service (API documentation at https://language-tools.ec.europa.eu/). Translations are stored in a database for reuse by the SWR. The EU translation returns asynchronous responses to translation requests, this means that translations may not yet be available after initial load of new data. A callback operation populates the database, from that moment a translation is available to SWR. The translation service uses 2-letter language codes, it means a translation from a 3-letter iso code (as used in for example iso19139:2007) to 2-letter code is required. The EU translation service has a limited set of translations from a certain to alternative language available, else returns an error.</p> <p>Initial translation is triggered by a running harvester. The translations will then be available once the record is ingested to the triplestore and catalogue database in a followup step of the harvester. </p>"},{"location":"technical_components/metadata_augmentation/#foreseen-functionality","title":"Foreseen functionality","text":"<p>In the next iterations, Metadata augmentation component is foreseen to include the following additional functions:</p> <ul> <li>Keyword matcher</li> <li>Spatial Locator</li> <li>Spatial scope analyser</li> <li>EUSO-high-value dataset tagging</li> </ul>"},{"location":"technical_components/metadata_augmentation/#keyword-matcher","title":"Keyword matcher","text":"<p>Keywords are an important mechanism to filter and cluster records. But similar keywords need to be equal to be able to match them. This module evaluates keywords of existing records to make them equal in case of high similarity. </p> <p>Analyses existing keywords on a metadata record. Two cases can be identified:</p> <ul> <li>If a keyword, having a skos identifier, has a closeMatch or sameAs relation to a prefered keyword, the prefered keyword is used. </li> <li>If an existing keyword, without skos identifier, matches a prefered keyword by (translated) string or synonym, then append the matched keyword (including skos identifier). Consider the risk of false positives.</li> </ul> <p>To facilitate this use case the SWR contains a knowledge graph of prefered keywords in the soil domain with relations to alternative keywords, such as agrovoc, gemet, dpedia, iso. This knowledge graph is maintained at https://github.com/soilwise-he/soil-health-knowledge-graph. Agrovoc is multilingual, facilitating the translation case.</p> <p>For metadata records which have not been analysed yet (in that iteration), the module extracts the records, for each keyword an analyses is made if it maches any of the prefered keywords, if so, the prefered keyword is added to the record. </p>"},{"location":"technical_components/metadata_augmentation/#spatial-locator","title":"Spatial Locator","text":"<p>Analyses existing keywords to find a relevant geography for the record, it then uses the GeoNames API to find spatial coordinates for the geography, which are inserted into the metadata record.</p>"},{"location":"technical_components/metadata_augmentation/#spatial-scope-analyser","title":"Spatial scope analyser","text":"<p>A script that analyses the spatial scope of a resource</p> <p>The bounding box is matched to country bounding boxes</p> <p>To understand if the dataset has a global, continental, national or regional scope</p> <ul> <li>Retrieves all datasets (as iso19139 xml) from database (records table joined with augmentations) which:<ul> <li>have a bounding box </li> <li>no spatial scope</li> <li>in iso19139 format</li> </ul> </li> <li>For each record it compares the boundingbox to country bounding boxes: <ul> <li>if bigger then continents &gt; global</li> <li>If matches a continent &gt; continental</li> <li>if matches a country &gt; national</li> <li>if smaller &gt; regional</li> </ul> </li> <li>result is written to as an augmentation in a dedicated table</li> </ul>"},{"location":"technical_components/metadata_augmentation/#euso-high-value-dataset-tagging","title":"EUSO-high-value dataset tagging","text":"<p>The EUSO high-value datasets are those with substantial potential to assess soil health status, as detailed on the EUSO dashboard. This framework includes the concept of soil degradation indicator metadata-based identification and tagging. Each dataset (possibly only those with the supra-national spatial scope - under discussion) will be annotated with a potential soil degradation indicator for which it might be utilised. Users can then filter these datasets according to their specific needs. </p> <p>The EUSO soil degradation indicators employ specific methodologies and thresholds to determine soil health status, see also the Table below. These methodologies will also be considered, as they may have an impact on the defined thresholds. This issue will be examined in greater detail in the future.</p> Soil Degradation Soil Indicator Type of methodic for threshold Soil erosion Water erosion RUSLE2015 Wind erosion GIS-RWEQ Tillage erosion SEDEM Harvest erosion Textural index Post-fire recovery USLE (Type of RUSLE) Soil pollution Arsenic excess GAMLSS-RF Copper excess GLM and GPR Mercury excess LUCAS topsoil database Zinc Excess LUCAS topsoil database Cadmium Excess GEMAS Soil nutrients Nitrogen surplus NNB Phosphorus deficiency LUCAS topsoil database Phosphorus excess LUCAS topsoil database Loss of soil organic carbon Distance to maximum SOC level qGAM Loss of soil biodiversity Potential threat to biological functions Expert Polling, Questionnaire, Data Collection, Normalization and Analysis Soil compaction Packing density Calculation of Packing Density (PD) Salinization Secondary salinization - Loss of organic soils Peatland degradation - Soil consumption Soil sealing Raster remote sense data <p>Technically, we forsee the metadata tagging process as illustrated below. At first, metadata record's title, abstract and keywords will be checked for the occurence of specific values from the Soil Indicator and Soil Degradation Codelists, such as <code>Water erosion</code> or <code>Soil erosion</code> (see the Table above). If found, the <code>Soil Degradation Indicator Tag</code> (corresponding value from the Soil Degradation Codelist) will be displayed to indicate suitability of given dataset for soil indicator related analyses. Additionally, a search for corresponding methodology will be conducted to see if the dataset is compliant with the EUSO Soil Health indicators presented in the EUSO Dashboard. If found, the tag <code>EUSO High-value dataset</code> will be added. In later phase we assume search for references to Scientific Methodology papers in metadata record's links. Next, the possibility of involving a more complex search using soil thesauri will also be explored.</p> <pre><code>flowchart TD\n    subgraph ic[Indicators Search]\n        ti([Title Check]) ~~~ ai([Abstract Check])\n        ai ~~~ ki([Keywords Check])\n    end\n    subgraph Codelists\n        sd ~~~ si\n    end\n    subgraph M[Methodologies Search]\n        tiM([Title Check]) ~~~ aiM([Abstract Check])\n        kl([Links check]) ~~~ kM([Keywords Check])\n    end\n    m[(Metadata Record)] --&gt; ic\n    m --&gt; M\n    ic-- + ---M\n    sd[(Soil Degradation Codelist)] --&gt; ic\n    si[(Soil Indicator Codelist)] --&gt; ic\n    em[(EUSO Soil Methodologies list)] --&gt; M\n    M --&gt; et{{EUSO High-Value Dataset Tag}}\n    et --&gt; m\n    ic --&gt; es{{Soil Degradation Indicator Tag}}\n    es --&gt; m\n    th[(Thesauri)]-- synonyms ---Codelists\n</code></pre>"},{"location":"technical_components/metadata_authoring/","title":"Metadata Authoring","text":"<p>Info</p> <p>Project: Soilinfohub</p> <p>Access point: https://github.com/soilwise-he/soilinfohub</p>"},{"location":"technical_components/metadata_authoring/#functionality","title":"Functionality","text":"<p>No implementations are yet an integrated part of the SWR delivery, as they were intentionally out of the first development itertation. Metadata authoring and generation is, however, possible using the hale\u00bbconnect workflows.</p>"},{"location":"technical_components/metadata_authoring/#foreseen-functionality","title":"Foreseen functionality","text":"<p>Users are enabled to create and maintain metadata records within the SWR, in case these records can not be imported from a remote source. Note that importing records from remote is the preferred approach from the SWR point of view because the ownership and persistence of the record is facilitated by the remote platform. </p> <ul> <li>Users login to the system and are enabled to upload a metadata record. </li> <li>A form is available for users to create or manage an existing record. The form has select options for those fields which are linked to a codelist. </li> <li>Users can also upload a spreadsheet of records which are converted to the MCF format.</li> <li>Users will see metadata validation results.</li> </ul>"},{"location":"technical_components/metadata_authoring/#technology","title":"Technology","text":"<p>The authoring workflow uses a GIT backend, additions to the catalogue are entered by members of the GIT repository directly or via pull request (review). Records are stored in iso19139:2007 XML or MCF. MCF is a subset of iso19139:2007 in a YAML encoding, defined by the pygeometa community. The pygeometa library is used to  convert the MCF to any requested metadata format.</p> <p>The pygeometa community provides a webbased form for users uncomfortable with editing an MCF file directly. The tool can be hosted within SWR, to faciliate a dedicated color scheme. The form is auto generated from mcf json schema, the schema can be annotated to provide a dedicated EUSO user experience (for example preselect relevant codelists).</p> <p>Users can also submit metadata using a CSV (excel) format, which is converted to MCF in a CI-CD workflow </p> <p>At intervals the SWR ingests metadata which has been uploaded via the authoring workflow.</p>"},{"location":"technical_components/metadata_validation/","title":"Metadata Validation","text":"<p>Metadata should help users assess the usability of a data set for their own purposes and help users to understand their quality.</p> <p>In terms of metadata, SoilWise Repository aims for the approach to harvest and register as much as possible (see more information in the Harvester Component). Catalogues which capture metadata authored by data custodians typically have a wide range of metadata completion and accuracy. Therefore, the SoilWise Repository employs metadata validation mechanisms to provide additional information about metadata completeness, conformance and integrity. Information resulting from the validation process are stored together with each metadata record in a relation database and updated after registering a new metadata version. Within the first iteration, they are not displayed in the SoilWise Catalogue, except of the results of the Link liveliness assessment component. For the following iterations, we forsee the validation results to be available only to data / knowledge owners / managers and the SWR admins, as SoilWise is not in an arbiter's role.</p> <p>After Metadata augmentation, the whole validation process can be repeated to understand the variability of metadata and value which has been added by SWR.</p> <p>Validations:</p> <ul> <li>Metadata profile validation</li> <li>Link liveliness assessment</li> </ul>"},{"location":"technical_components/metadata_validation/#metadata-profiles","title":"Metadata profiles","text":"<p>Metadata profiles specify the required metadata elements that must be included to describe resources, ensuring they are discoverable, accessible, and usable. Metadata validation is inherently linked to the specific metadata profile it is intended to follow. This linkage ensures that metadata records are consistent, meet the necessary standards, and are fit for their intended purpose, thereby supporting effective data management, discovery, and use. In the soil domain, several metadata profiles are commonly used to ensure the effective documentation, discovery, and utilization of soil data, for example Datacite, GBIF-EML, Geo-DCAT-AP, INSPIRE Metadata Profile, Dublin Core, ANZLIC Metadata Profile, FAO Global Soil Partnership Metadata Profile, EJP/EUSO Metadata Profile. SoilWise Repository is currently able to perform validations according to the following metadata profiles:</p>"},{"location":"technical_components/metadata_validation/#euso-metadata-profile","title":"EUSO Metadata profile","text":"<p>This metadata profile was developed through EJP Soil project efforts and modified and approved by the EUSO Working Group.</p> <p>This metadata profile has been used within the first development iteration phase. Its further modification are under discussions among all the stakeholders.</p> Label Cardinality Codelist Description Identification 1-n Unique identification of the dataset (A UUID, URN, or URI, such as DOI) Title 1-1 Short meaningful title Abstract 1-1 Short description or abstract (1/2 page), can include (multiple) scientific/technical references Extent (geographic) 0-1 BBOX or Geonames Geographical coverage (e.g. EU, EU &amp; Balkan, France, Wallonia, Berlin) Reference period - Start 0-1 Reference period for the data - Start Reference period - End 0-1 Reference period - End; empty if ongoing Access constraints 1-1 INSPIRE Indicates if the data is publicly accessible or the reason to apply access constaints Usage constraints 1-1 INSPIRE Indicates if there are legal usage constraints (license) Keywords 0-n Keywords Contact 1-n name; organisation; email; role, where role is one of distributor, owner, pointOfContact, processor, publisher, metadata-contact Source 0-n Source is a reference to another dataset which is used as a source for this dataset. Reference a single dataset per line; Title; Date; or provide a DOI; isSourceOf 0-n Other datasets that the current dataset is used as input source Lineage 1-1 Statement on the origin and processing of the data Processing steps 0-n Methods applied in data acquisition and processing: preferably reference a method from a standard (national, LUCAS, FAO, etc.). One processing step per line; Method; Date; Processor; Method reference; Comment Language 1-n ISO Language, of the data and metadata, if metadata is multilingual multiple languages can be provided Reference system 0-1 CRS Spatial Projection: drop down list of options, including \u2018unknown\u2019\u00a0\u00a0(you can also leave out the field if it is unknown) Citation 0-n Citations are references to articles which reference this dataset; one citation on each line; Title; Authors; Date; or provide a DOI Spatial resolution 0-n Resolution (grid) or scale (vector) Data type 0-1 table, vector, grid The type of data Geometry type 0-1 point, line, polygon, ... Geometry type for vector data File / service Location 0-n Url or path to the data file or service Format 0-n IANA File Format in which the data is maintained or published Delivery 0-n The  way the dataset is available (ie digital: download, viewer OR physical way: Shipping or in situ access ) Maintenenance frequency 0-1 ISO Indication of the frequency of data updates Modification date 0-1 Date of last modification Status 0-1 ISO Status of the dataset Subject - Spatial scope 0-n INSPIRE The scope of the dataset, e.g. regional, national, continental Subject - Soil properties 0-n INSPIRE Soil properties described in this dataset Subject - Soil function 0-n INSPIRE Soil funtions described in this dataset Subject - Soil threats 0-n INSPIRE Soil threats described in this dataset Subject - Soil Indicators 0-n INSPIRE Soil indicators  described in this dataset Subject - EUSO Data WG subgroup 0-n EUSO The EUSO subgroups which contributed to this record Subject - Context 0-n EUSO Context: (e.g. EU-Project SOILCARE, EJP-Soil, Literature, ESDAC, etc.)\u00a0 Subject - Possible End-users 0-n EUSO Possible end-users: citizens, scientific community, private sector, EU, member states, academia Subject - Category 0-n EUSO One or more thematic categories of the dataset Quality statement 0-1 A statement of quality or any other supplemental information Datamodel/dimensions 0-1 The datamodel (table) or dimensions (grid) of the dataset Units of measure 0-n ISU List of UoM from International System of Units, at attribute/dimension level Attribute type 0-n string, number, date The type of attribute Categorical Data 0-n Lookup tables for categorical data, at attribute/dimension level Uncertainty 0-n Method used to assess uncertainty and its result. For example: One or more measurements to describe the error and uncertainties in the dataset Completeness 0-1 The % of completeness"},{"location":"technical_components/metadata_validation/#inspire-metadata-profile","title":"INSPIRE metadata profile","text":"<p>The validation against the INSPIRE metadata profile checks whether the metadata records are in accordance with the technical requirements of INSPIRE, specifically according to the INSPIRE data specification on Soil \u2013 Technical Guidelines version 3.0. The Soil-specific metadata elements are: </p> Type Package Stereotypes DerivedProfilePresenceInSoilBody \u00abassociationType\u00bb DerivedSoilProfile \u00abfeatureType\u00bb FAOHorizonMasterValue \u00abcodelist\u00bb FAOHorizonNotationType \u00abdataType\u00bb FAOHorizonSubordinateValue \u00abcodelist\u00bb FAOPrimeValue \u00abcodelist\u00bb LayerGenesisProcessStateValue \u00abcodelist\u00bb LayerTypeValue \u00abcodelist\u00bb ObservedSoilProfile \u00abfeatureType\u00bb OtherHorizonNotationType \u00abdataType\u00bb OtherHorizonNotationTypeValue \u00abcodelist\u00bb OtherSoilNameType \u00abdataType\u00bb OtherSoilNameTypeValue \u00abcodelist\u00bb ParticleSizeFractionType \u00abdataType\u00bb ProfileElement \u00abfeatureType\u00bb ProfileElementParameterNameValue \u00abcodelist\u00bb RangeType \u00abdataType\u00bb SoilBody \u00abfeatureType\u00bb SoilDerivedObject \u00abfeatureType\u00bb SoilDerivedObjectParameterNameValue \u00abcodelist\u00bb SoilHorizon \u00abfeatureType\u00bb SoilInvestigationPurposeValue \u00abcodelist\u00bb SoilLayer \u00abfeatureType\u00bb SoilPlot \u00abfeatureType\u00bb SoilPlotTypeValue \u00abcodelist\u00bb SoilProfile \u00abfeatureType\u00bb SoilProfileParameterNameValue \u00abcodelist\u00bb SoilSite \u00abfeatureType\u00bb SoilSiteParameterNameValue \u00abcodelist\u00bb SoilThemeCoverage \u00abfeatureType\u00bb SoilThemeDescriptiveCoverage \u00abfeatureType\u00bb SoilThemeDescriptiveParameterType \u00abdataType\u00bb SoilThemeParameterType \u00abdataType\u00bb WRBQualifierGroupType \u00abdataType\u00bb WRBQualifierPlaceValue \u00abcodelist\u00bb WRBQualifierValue \u00abcodelist\u00bb WRBReferenceSoilGroupValue \u00abcodelist\u00bb WRBSoilNameType \u00abdataType\u00bb WRBSpecifierValue \u00abcodelist\u00bb"},{"location":"technical_components/metadata_validation/#functionality","title":"Functionality","text":""},{"location":"technical_components/metadata_validation/#metadata-profile-validation","title":"Metadata profile validation","text":"<p>Info</p> <p>Current version: 0.1.0</p> <p>Project: Metadata validator</p> <p>Access point: https://data.soilwise.wetransform.eu/#/home (authorization needed)</p>"},{"location":"technical_components/metadata_validation/#metadata-structure-validation","title":"Metadata structure validation","text":"<p>The initial steps of metadata validation comprise:</p> <ol> <li>Syntax Check: Verifying that the metadata adheres to the specified syntax rules. This includes checking for allowed tags, correct data types, character encoding, and adherence to naming conventions.</li> <li>Schema (DTD/xsd/shacl/json-schema) Validation: Ensuring that the metadata conforms to the defined schema or metadata model. This involves verifying that all required elements are present, and relationships between different metadata components are correctly established.</li> </ol>"},{"location":"technical_components/metadata_validation/#metadata-completeness-indication","title":"Metadata completeness indication","text":"<p>The indication calculates a level of completeness of a record, indicated in % of 100 for endorsed properties of the EUSO soil profile, considering that some properties are conditional based on selected values in other properties.</p>"},{"location":"technical_components/metadata_validation/#metadata-etsats-checking","title":"Metadata ETS/ATS checking","text":"<p>The methodology of ETS/ATS has been suggested to develop validation tests.</p> <p>Abstract Executable Test Suites (ATS) define a set of abstract test cases or scenarios that describe the expected behaviour of metadata without specifying the implementation details. These test suites focus on the logical aspects of metadata validation and provide a high-level view of metadata validation requirements, enabling stakeholders to understand validation objectives and constraints without getting bogged down in technical details. They serve as a valuable communication and documentation tool, facilitating collaboration between metadata producers, consumers, and validators. ATS are often documented using natural language descriptions, diagrams, or formal specifications. They outline the expected inputs, outputs, and behaviours of the metadata under various conditions.</p> <p>Executable Test Suites (ETS) are sets of tests designed according to ATS to perform the metadata validation. These tests are typically automated and can be run repeatedly to ensure consistent validation results. Executable test suites consist of scripts, programs, or software tools that perform various validation checks on metadata. These checks can include:</p> <ol> <li>Data Integrity: Checking for inconsistencies or errors within the metadata. This includes identifying missing values, conflicting information, or data that does not align with predefined constraints.</li> <li>Standard Compliance: Assessing whether the metadata complies with relevant industry standards, such as Dublin Core, MARC, or specific domain standards like those for scientific data or library cataloguing.</li> <li>Interoperability: Evaluating the metadata's ability to interoperate with other systems or datasets. This involves ensuring that metadata elements are mapped correctly to facilitate data exchange and integration across different platforms.</li> <li>Versioning and Evolution: Considering the evolution of metadata over time and ensuring that the validation process accommodates versioning requirements. This may involve tracking changes, backward compatibility, and migration strategies.</li> <li>Quality Assurance: Assessing the overall quality of the metadata, including its accuracy, consistency, completeness, and relevance to the underlying data or information resources.</li> <li>Documentation: Documenting the validation process itself, including any errors encountered, corrective actions taken, and recommendations for improving metadata quality in the future.</li> </ol>"},{"location":"technical_components/metadata_validation/#technology-integration","title":"Technology &amp; Integration","text":"<p>hale\u00bbconnect has been deployed. This platform includes the European Testing Framework ETF and can execute Metadata and Data validation usign the ETS approach outlined above. The User Guide is available here. The administration console of the platform can be accessed upon login at: https://data.soilwise.wetransform.eu/#/home.</p> <p>The metadata validation component will show its full potential when integrated to (1) SWR Catalogue, (2) Storage of metadata, and (3) Requires authentication and authorisation.</p>"},{"location":"technical_components/metadata_validation/#user-guide","title":"User Guide","text":"<p>When using the \u2018Metadata only\u2019 workflow, the metadata profile can be validated with hale\u00bbconnect. To do this, after logging into hale\u00bbconnect, go directly to the setup of a new Theme (transformation project and Schema are therefore not required) and activate \u2018Publish metadata only\u2019 and specify where the metadata should come from. To validate the metadata file, upload the metadata and select \u2018Metadata only\u2019. Once validation is complete, a report can be called up.</p> <p>A comprehensive tutorial video on setting up and executing transformation workflows can be found here.</p>"},{"location":"technical_components/metadata_validation/#future-work","title":"Future work","text":"<ul> <li>full development of the ETS, using populated codelists,</li> <li>display validation results in the SoilWise Catalogue,</li> <li>on-demand metadata validation, which would generate reports for user-uploaded metadata,</li> <li>applicability of ISO19157 Geographic Information \u2013 Data quality (i.e. the standard intended for data validations) for metadata-based validation reports,</li> <li>Shacl is is in general intended for semantic web related validations; however, it's exact scope will be determined during the upcoming SoilWise developments. </li> </ul>"},{"location":"technical_components/metadata_validation/#link-liveliness-assessment","title":"Link liveliness assessment","text":"<p>Info</p> <p>Current version: 0.1.0</p> <p>Projects: Link liveliness assessment</p> <p>Metadata (and data and knowledge sources) tend to contain links to other resources. Not all of these URIs are persistent, so over time they can degrade. In practice, many non-persistent knowledge sources and assets exist that could be relevant for SWR, e.g. on project websites, in online databases, on the computers of researchers, etc. Links pointing to such assets might however be part of harvested metadata records or data and content that is stored in the SWR. </p> <p>The link liveliness assessment subcomponent runs over the available links stored with the SWR assets and checks their status. The function is foreseen to run frequently over the URIs in the SWR repository, assessing and storing the status of the link. The link liveliness  privides the following functions:</p> <ol> <li>OGC API Catalogue Integration<ul> <li>Designed to work specifically with OGC API - Records System</li> <li>Extracts and evaluates URLs from catalogue items </li> </ul> </li> <li>Link Validation<ul> <li>Evaluates the validity of links to external sources and within the repository</li> <li>Checks if metadata accurately represents the source</li> </ul> </li> <li>Support for OGC service links<ul> <li>Identifies and properly handles OGC service links (WMS, WFS, CSW, WCS etc.) before assessing them</li> </ul> </li> <li>Health Status Tracking<ul> <li>Provides up-to-date status history for every assessed link</li> <li>Maintains a history of link health over time</li> </ul> </li> <li>Flexible Evaluation<ul> <li>Supports single resource evaluation on demand</li> <li>Performs periodic tests to provide availability history</li> </ul> </li> <li>Broken link management<ul> <li>Identifies and categorizes broken links based on their status code ( <code>401 Unauthorized</code>, <code>404 Not Found</code>, <code>500 Server Error</code>)</li> <li>Flags deprecated links after consecutive failed tests and excludes them from future check</li> </ul> </li> <li>Timeout management<ul> <li>Identifies resources exceeding specified timeout thresholds</li> </ul> </li> </ol> <p>A javascript widget is further used to display the link status directly in the SWR Catalogue record.</p> <p></p>"},{"location":"technical_components/metadata_validation/#technology","title":"Technology","text":"<ul> <li>Python         Used for the linkchecker integration, API development, and database interactions</li> <li>PostgreSQL         Primary database for storing and managing link information</li> <li>FastAPI         Employed to create and expose REST API endpoints.          Utilizes FastAPI's efficiency and auto-generated Swagger documentation</li> <li>Docker          Used for containerizing the application, ensuring consistent deployment across environments</li> <li>CI/CD         Automated pipeline for continuous integration and deployment, with scheduled weekly runs for link liveliness assessment</li> </ul>"},{"location":"technical_components/monitoring/","title":"System Usage &amp; Monitoring","text":"<p>Info</p> <p>Current version: 1.0</p> <p>Project: Usasge statistics</p> <p>All components and services of the SWR are monitored at different levels to ensure robust operations and security of the system. There will be a central monitoring service for all components that are part of the SWR.</p> <p>In particular, monitoring needs to fulfill the following requirements:</p> <ul> <li>For each node, its general state and resource utilisation (RAM, CPU, Volumes) shall be monitored.</li> <li>For each container, its general state, e.g. resource consumption (RAM, CPU, Volumes, Transfer, Uptime) shall be monitored.</li> <li>For each service, there shall be a health check that can be used to test if the service is responsive and functional, e.g. after a restart.</li> <li>If issues that cannot be recovered from automatically occur or which lead to a longer-term degradation of services, messages shall be sent to the operators via channels such as Slack, PagerDuty, or Jira.</li> <li>The monitoring system shall provide availability statistics.</li> <li>The monitoring system should provide usage statistics.</li> <li>The monitoring system may provide a UI element that can be embedded into other components to make usage transparent.</li> <li>The monitoring system should provide a dashboard to help system operators with understanding the state of the SWR and to debug incidents, including possible security incidents.</li> <li>The monitoring system shall collect warning and error logs to provide guidance for system administrators.</li> <li>The monitoring system shall offer the possibility to filter logged interactions based on the https status code, e.g. to identify 404's or 500's.</li> </ul>"},{"location":"technical_components/monitoring/#system-context-and-implementation-hints-for-monitoring","title":"System context and implementation hints for monitoring","text":"<ul> <li>Monitoring connects with: all modules</li> <li>Technologies used: Grafana, Portainer, Prometheus</li> <li>External intgrations: Jira, Slack, PagerDuty, ... </li> </ul>"},{"location":"technical_components/natural_language_querying/","title":"Natural Language Querying","text":"<p>Making open knowledge findable and accessible for SoilWise users</p> <p>Info</p> <p>Current version: 0.1.0</p> <p>Project: Natural Language querying</p>"},{"location":"technical_components/natural_language_querying/#functionality","title":"Functionality","text":"<p>The aplication of Natural Language Querying (NLQ) for SoilWise and the integration into the SoilWise repository is currently still in the research phase. No implementations are yet an integrated part of the SWR delivery, in line with the plan for the first development iteration.</p>"},{"location":"technical_components/natural_language_querying/#ongoing-developments","title":"Ongoing Developments","text":"<p>A strategy for development and implementation of NLQ to support SoilWise users is currently being developed. It considers various ways to make knowledge available through NLQ, possibly including options to migrate to different \"levels\" of complexity and innovation.</p> <p>Such a \"leveled approach\" could start from leveraging existing/proven search technology (e.g. the Apache Solr open source search engine), and gradually combining this with new developments in NLP (such as transformer based language models) to make harvested knowledge metadata and harmonized knowledge graphs accessible to SoilWise users. </p> <p>Typical general steps towards an AI-powered self-learning search system, are listed below from less to more complex. Note that to fully benefit from later steps it will be necessary to process knowledge (documents) themselves (\"look inside the documents\") instead of only working with the metadata about them. </p> <ul> <li>basic keyword based search (tf-idf<sup>4</sup>, bm25<sup>5</sup>)</li> <li>use of taxonomies and entity extraction</li> <li>understanding query intent (semantic query parsing, semantic knowledge graphs, virtual assistants)</li> <li>automated relevance tuning (signals boosting, collaborative filtering, learning to rank)</li> <li>Self-learning search system (full feedback loop using all user and content data)</li> </ul> <p></p> <p>Core topics are:</p> <ul> <li>LLM<sup>1</sup> based (semantic) KG generation from unstructured content (leveraging existing search technology)</li> <li>chatbot - Natural Language Interface (using advanced NLP<sup>2</sup> methodologies, such as LLMs)</li> <li>LLM operationalisation (RAG<sup>3</sup> ingestion pipeline(s), generation pipeline, embedding store, models)</li> </ul> <p>The final aim is towards extractive question answering (extract answers from sources in real-time), result summarization (summarize search results for easy review), and abstractive question answering (generate answers to questions from search results). Not all these aims might be achievable within the project though. Later steps (marked in yellow in the following image) depend more on the use of complex language models.</p> <p></p> <p>One step towards personalisation could be the use of (user) signals boosting and collaborative filtering. But this would require tracking and logging (user) actions.</p> <p>A seperate development could be a chatbot based on selected key soil knowledge documents ingested into a vector database (as a fixed resource), or even a fine-tuned LLM that is more soil science specific than a plain foundation LLM.</p> <p>Optionally the functionality can be extended from text processing to also include multi-modal data such as photos (e.g. of soil profiles). Effort needed for this has to be carefully considered.</p> <p>Along the way natural language processing (NLP) methods and approaches can (and are) also be applied for various metadata handling and augmentation.</p>"},{"location":"technical_components/natural_language_querying/#foreseen-technology","title":"Foreseen technology","text":"<ul> <li>(Semantic) search engine, e.g. Apache Solr or Elasticsearch</li> <li>Graph database (if needed)</li> <li>(Scalable) vector database (if needed)</li> <li>Java and/or Python based NLP libraries, e.g. OpenNLP, spaCy</li> <li>Small to large foundation LLMs</li> <li>LLM development framework (such as langChain or LlamaIndex)</li> <li>Frontend toolkit </li> <li>LLM deployment and/or hosted API access</li> <li>Authentication and authorisation layer</li> <li>Computation and storage infrastructure</li> <li>Hardware acceleration, e.g. GPU (if needed)</li> </ul> <ol> <li> <p>Large Language Model. Typically a deep learning model based on the transformer architecture that has been trained on vast amounts of text data, usually from known collections scraped from the Internet.\u00a0\u21a9</p> </li> <li> <p>Natural Language Processing. An interdisciplinary subfield of computer science and artificial intelligence, primarily concerned with providing computers with the ability to process data encoded in natural language. It is closely related to information retrieval, knowledge representation and computational linguistics.\u00a0\u21a9</p> </li> <li> <p>Retrieval Augmented Generation. A framework for retrieving facts from an external knowledge base to ground large language models on the most accurate, up-to-date information and enhancing the (pre)trained parameteric (semantic) knowledge with non-parameteric knowledge to avoid hallucinations and get better responses.\u00a0\u21a9</p> </li> <li> <p>tf-idf. Term Frequency - Inverse Document Frequency, a statistical method in NLP and information retrieval that measures how important a term is within a document relative to a collection of documents (called a corpus).\u00a0\u21a9</p> </li> <li> <p>bm25. Okapi Best Match 25, a well-known ranking function used by search engines to estimate the relevance of documents to a given search query. It is based on tf-idf, but considered an improvement and adding some tunable parameters.\u00a0\u21a9</p> </li> </ol>"},{"location":"technical_components/storage/","title":"Repository Storage","text":"<p>Info</p> <p>Current version: Postgres release 12.2; Virtuoso release 07.20.3239</p> <p>Access point: Triple Store (SWR SPARQL endpoint) https://sparql.soilwise-he.containers.wur.nl/sparql</p> <p>The SoilWise repository aims at merging and seamlessly providing different types of content. To host this content and to be able to efficiently drive internal processes and to offer performant end user functionality, different storage options are implemented.</p> <ol> <li>A relational database management system for the storage of the core metadata of both data and knowledge assets.</li> <li>A Triple Store to store the metadata of data and knowledge assets as a graph, linked to soil health and related knowledge as a linked data graph.</li> <li>Git for storage of user-enhanced metadata.</li> </ol>"},{"location":"technical_components/storage/#functionality","title":"Functionality","text":""},{"location":"technical_components/storage/#postgress-rdbms-storage-of-raw-and-augmented-metadata","title":"Postgress RDBMS: storage of raw and augmented metadata","text":"<p>A \"conventional\" RDBMS is used to store the (augmented) metadata of data and knowledge assets. The harvester process uses it to store the raw results of the metadata harvesting of the different resources that are currently connected. Various metadata augmentation jobs use it as input and write their input to this data store. The catalogue also queries the Postgress database. </p> <p>There are several reasons for choosing an RDBMS as the main source for metadata storage and metadata querying</p> <ul> <li>An RDBMS provides good options to efficiently structure and index its contents, thus allowing performant access for both internal processes and end user interface querying.</li> <li>An RDBMS easily allows implementing constraints and checks to keep data and relations consistent and valid.</li> <li>Various extensions, e.g. search engines, are available to make querying, aggregations even more performant and fitted for end users.</li> </ul>"},{"location":"technical_components/storage/#virtuoso-triple-store-storage-of-swr-knowledge-graph","title":"Virtuoso Triple Store: storage of SWR knowledge graph","text":"<p>A Triple Store is implemented as part of the SWR infrastructure to allow a more flexible linkage between the knowledge captured as metadata and various sources of internal and external knowledge sources, particularly taxonomies, vocabularies and ontologies that are implemented as RDF graphs. Results of the harvesting and metadata augmentation that are stored in the RDBMS are converted to RDF and stored in the Triple Store. </p> <p>A Triple Store is selected as a parallel storage because it offers several capabilites </p> <ul> <li>It allows the linking of different knowledge models, e.g. to connect the SWR metadata model with existing and new knowledge structures on soil health and related domains.</li> <li>It allows reasoning over the relations in the stored graph, and thus allows connecting and smartly combining knowledge from those domains.</li> <li>Through the SPARQL interface, it allows users and processes to use such reasoning and exploit previously unconnected sets of knowledge.</li> </ul>"},{"location":"technical_components/storage/#git-user-enhanced-metadata","title":"Git: User enhanced metadata","text":"<p>The current setup of SWR, using the pycsw infrastructure, allows users to propose metadata enhancements. Such enhancements are managed in Git at: https://github.com/soilwise-he/soilinfohub/discussions.</p>"},{"location":"technical_components/storage/#ongoing-developments","title":"Ongoing Developments","text":"<p>In the next iteration of the SWR development, the currently deployed storage options will be extended to support new features and functions. Such extensions can improve performance and usability. Moreover, we expect that the integration of AI/ML based functions will require additional types of storage and better a integration to exploit their combined power. Exploratory work that was performed, but is not yet integrated into the deployment of iteration 1 include:</p>"},{"location":"technical_components/storage/#establishing-a-vector-database","title":"Establishing a vector database","text":"<p>A vector database is foreseen as a foundation to use Large Language Models (LLM) and implement Natural Language Querying (NLQ), e.g. to allow chatbot functionality for end users. A vector DB allows storage of text embeddings that are a the basis for such NLQ functions.</p>"},{"location":"technical_components/storage/#selecting-a-search-engine","title":"Selecting a search engine","text":"<p>A search engine, deployed on top of the current RDBMS, will increase the perfomance of end user queries. It can also offer better usability, e.g. by offering aggregation functions for faceted search and ranking of search results. Additionally, search engines are also implementing the indexation of unstructured content and are moving to supporting text embeddings. Thus, they might be a starting point (or alternative?) to offer smart searches on unstructured text, using more conventional and broadly adopted software and offering easier migration pathways towards NLQ-like functions. </p>"},{"location":"technical_components/storage/#technology-integration","title":"Technology &amp; Integration","text":"<p>Components used:</p> <ul> <li>Virtuoso (version 07.20.3239)</li> <li>PostgreSQL (release 12.13)</li> </ul>"},{"location":"technical_components/technical_components/","title":"Introduction","text":"<p>The SoilWise Repository (SWR) architecture aims towards efficient facilitation of soil data management. It seamlessly gathers, processes, and disseminates data from diverse sources. The system prioritizes high-quality data dissemination, knowledge extraction and interoperability while user management and monitoring tools ensure secure access and system health. Note that, SWR primarily serves to power Decision Support Systems (DSS) rather than being a DSS itself.</p> <p>The presented architecture represents an outlook and a framework for ongoing SoilWise development. As such, the implementation has been following intrinsic (within the SoilWise project) and extrinsic (e.g. EUSO development Mission Soil Projects) opportunities and limitations. The presented architecture is the first release out of two planned. Modifications during the implementation will be incorporated into the final version of the SoilWise architecture due M42.</p> <p>This section lists technical components for building the SoilWise Repository as forseen in the architecture design. As for now, the following components are foreseen:</p> <ol> <li>Harvester</li> <li>Repository Storage</li> <li>Catalogue</li> <li>Metadata Validation</li> <li>Metadata Authoring</li> <li>Transformation and Harmonistation</li> <li>Metadata Augmentation</li> <li>Knowledge Graph</li> <li>Natural Language Querying</li> <li>User Management and Access Control</li> </ol> <p>A full version of architecture diagram is available at: https://soilwise-he.github.io/soilwise-architecture/.</p>"},{"location":"technical_components/transformation/","title":"Transformation and Harmonisation","text":"<p>Info</p> <p>Current version: 5.3</p> <p>Project: Hale Studio</p> <p>These components make sure that data is interoperable, i.e. provided to agreed-upon formats, structures and semantics. They are used to ingest data and transform it into common standard data, e.g. in the central SWR format for soil.</p> <p>The specific requirements these components have to fulfil are:</p> <ul> <li>The services shall be able to work with data that is described explicitly or implicitly with a schema. The services shall be able to load schemas expressed as XML Schemas, GML Application Schemas, RDF-S and JSON Schema.</li> <li>The services shall support GML, GeoPackage, GeoJSON, CSV, RDF and XSL formats for data sources.</li> <li>The services shall be able to connect with external download services such as WFS or OGC API, Features.</li> <li>The services shall be able to write out data in GML, GeoPackage, GeoJSON, CSV, RDF and XSL formats.</li> <li>There shall be an option to read and write data from relational databases.</li> <li>The services should be exposed as OGC API Processes</li> <li>Transformation processes shall include the following capabilities:<ul> <li>Rename types &amp; attributes.</li> <li>Convert between units of measurement.</li> <li>Restructure data, e.g. through, joining, merging, splitting.</li> <li>Map codelists and other coded values.</li> <li>Harmonise observations as if they were measured using a common procedure using Pedotransfer Functions.</li> <li>Reproject data.</li> <li>Change data from one format to another.</li> </ul> </li> <li>There should be an interactive editor to create the specific transformation processes required for the SWR.</li> <li>It should be possible to share transformation processes.</li> <li>Transformation processes should be fully documented or self-documented.</li> </ul>"},{"location":"technical_components/transformation/#technology-integration","title":"Technology &amp; Integration","text":"<p>We have deployed the following components to the SWR infrastructure:</p> <ul> <li>hale studio, a proven ETL tool optimised for working with complex structured data, such as XML, relational databases, or a wide range of tabular formats. It supports all required procedures for semantic and structural transformation. It can also handle reprojection. While Hale Studio exists as a multi-platform interactive application, its capabilities can be provided through a web service with an OpenAPI.</li> <li>A comprehensive tutorial video on soil data harmonisation with hale studio can be found here</li> </ul> <p>Another part of the deployed system, GDAL, a very robust conversion library used in most FOSS and commercial GIS software, can be used for  a wealth of format conversions and can handle reprojection. In cases where no structural or semantic transformation is needed, a GDAL-based conversion service would make sense. </p>"},{"location":"technical_components/transformation/#setting-up-a-transformation-process-in-haleconnect","title":"Setting up a transformation process in hale\u00bbconnect","text":"<p>Complete the following steps to set up soil data transformation, validation and publication processes:</p> <ol> <li>Log into hale\u00bbconnect.</li> <li>Create a new transformation project (or upload it).</li> <li>Specify source and target schemas.</li> <li>Create a theme (this is a process that describes what  should happen with the data).</li> <li>Add a new transformation configuration. Note: Metadata generation can be configured in this step.</li> <li>A validation process can be set up to check against conformance classes.</li> </ol>"},{"location":"technical_components/transformation/#executing-a-transformation-process","title":"Executing a transformation process","text":"<ol> <li>Create a new dataset and select the theme of the current source data, and provide the source data file.</li> <li>Execute the transformation process. ETF validation processes are also performed. If successful, a target dataset and the validation reports will be created.</li> <li>View and download services will be created if required.</li> </ol> <p>To create metadata (data set and service metadata), activate the corresponding button(s) when setting up the theme for the transformation process.</p>"},{"location":"technical_components/user_management/","title":"User Management and Access Control","text":"<p>User and organisation management, authorisation and authentication are complex, cross-cutting aspects of a system such as the SoilWise repository. Back-end and front-end components need to perform access control for authenticated users. Many organisations already have infrastructures in place, such as an Active Directory or a Single Sign On based on OAuth.</p> <p>No implementations are yet an integrated part of the SWR delivery, in line with the plan for the first development iteration.</p> <p>The general model we apply is that:</p> <ul> <li>a user shall be a member of at least one organisation.</li> <li>a user may have at least one role in every organisation that they are a member of.</li> <li>a user always acts in the context of one of their roles in one organisation (similar to Github contexts).</li> <li>organisations can be hierarchical, and user roles may be inherited from an organisation that is higher up in the hierarchy.</li> </ul> <p>The basic requirements for the SWR authentication mechanisms are:</p> <ul> <li>User authentication, and thus, provision of authentication tokens, shall be distributed (\"Identity Brokering\") and may happen through existing services. Authentication mechanisms that are to be supported include OAuth, SAML 2.0 and Active Directory.</li> <li>An authoritative Identity Provider, such as an eIDAS-based one, should be integrated in a later iteration as well.</li> <li>There shall be a central service that performs role and organisation mapping for authenticated users. This service also provides the ability to configure roles and set up organisations and users. This central service can also provide simple, direct user authentication (username/password-based) for those users who do not have their own authentication infrastructure.</li> <li>There may be different levels of trust establishment based on the specific authentication service used. Higher levels of trust may be required to access critical data or infrastructure.</li> <li>SWR services shall use Keycloak or JSON Web Tokens  for authorization.</li> <li>To access SWR APIs, the same rules apply as to access the SWR through the UI.</li> </ul> <p>In later iterations, the authentication and authorisation mechanisms should also be used to facilitate connector-based access to data space resources.</p>"},{"location":"technical_components/user_management/#sign-up","title":"Sign-up","text":"<p>For every registered user of SWR components, an account is needed. This account can be created in one of three ways:</p> <ol> <li>Automatically, by providing an authentication token that was created by a trusted authentication service and that contains the necessary information on the organisation of the user and the intended role (this can e.g. be implemented through using a DAPS)</li> <li>Manually, through self-registration (may only be available for users from certain domains and/or for certain roles)</li> <li>Through superuser registration; in this case the user gets issued an activation link and has to set the password to complete registration</li> </ol>"},{"location":"technical_components/user_management/#authentication","title":"Authentication","text":"<p>Certain functionalities of the SWR will be available to anonymous users, but functions that edit any of the state of the system (data, configuration, metadata) require an authenticated user. The easiest form of authentication is to use the login provided by the SWR itself. This log-in is username-password based. A second factor, e.g. through an authenticator app, may be added in the upcoming iteration.</p> <p>Other forms of authentication include using an existing token.</p>"},{"location":"technical_components/user_management/#authorisation","title":"Authorisation","text":"<p>Every component has to check whether an authenticated user may invoke a desired action based on that user's roles in their organisations. To ensure that the User Interface does not offer actions that a given user may not invoke, the user interface shall also perform authorisation.</p> <p>Roles are generally defined using Privileges: A certain role may, for example, <code>read</code> certain resources, they may <code>edit</code> or even <code>delete</code> them. Here is an example of such a definition:</p> <p>A standard <code>user</code> may only <code>read</code> and <code>edit</code> their own <code>User</code> profile, and read the information from their organisation. Once a user has been given the role <code>dataManager</code>, they may perform any CRUD operation on any <code>Data</code> that is in the scope of their <code>organisation</code>. They are also granted <code>read</code> access to publication <code>Theme</code> configurations on their own and in any parent organisations.</p>"},{"location":"technical_components/user_management/#further-implementation-hints-and-technologies","title":"Further implementation hints and Technologies","text":"<p>The public cloud hale connect user service can be used for central user management.</p>"},{"location":"technical_components/user_management/#completed-work-iteration-1","title":"Completed work - Iteration 1","text":"<ul> <li>User/Role and Organisation management has been deployed and configured as part of weTransform's hale connect installation.</li> <li>As of now, there are three Identity providers deployed as part of that infrastructure:<ul> <li>The integrated user service in hale connect,</li> <li>a Keycloak/OpenID-connect based one using GoPass via Github</li> <li>a Data Spaces connector.</li> </ul> </li> </ul>"},{"location":"technical_components/user_management/#planned-work-iteration-2","title":"Planned work - Iteration 2","text":"<ul> <li>Integrate eIDAS or a different autheoritative Identity Provider</li> <li>Update other components to accept the tokens generated by this infrastructure</li> </ul>"}]}